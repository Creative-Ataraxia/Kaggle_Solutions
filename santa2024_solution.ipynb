{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏆 Santa 2024 - The Perplexity Permutation Puzzle\n",
    "\n",
    "* Author: **Roy Ma** (*creative-ataraxia*)\n",
    "* Date: Feb 1st, 2025\n",
    "* Objective: Optimize 6 string samples to achieve the lowest LM perplexity rating\n",
    "* Results: placed **7.27%** on the leaderboard, `sample_0, 1, 2 and 5` achieve a score on par within top 3 level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libs\n",
    "import argparse\n",
    "import copy\n",
    "import gc\n",
    "import hashlib\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import subprocess\n",
    "from time import time\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Generator, List, Optional, Union\n",
    "\n",
    "# 3rd parties libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import transformers\n",
    "from scipy.stats import spearmanr\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"                                      # use a single thread\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"                           # prevent race conditions\n",
    "PAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index            # prevent special tokens to be used for scoring\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    # check GPU\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines the various paths, and asserts input data's consistency with competition formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_MODEL = Path(\"I:/Models/LLMs/gemma_2_9b\")             # path to the local llm used for scoring\n",
    "PATH_SAMPLE_INPUT = Path(\"../input/sample_submission.csv\") # path to Kaggle default sample input\n",
    "DF_SAMPLE = pd.read_csv(PATH_SAMPLE_INPUT)                    \n",
    "\n",
    "PATH_SAVE = Path(\"./1st_save\")                             # save location for various models checkpoints and search results\n",
    "PATH_SAVE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NUM_SAMPLES = len(DF_SAMPLE)\n",
    "assert NUM_SAMPLES == 6                                    # total number of samples should be 6\n",
    "\n",
    "LIST_NUM_WORDS = [len(DF_SAMPLE.loc[sample_id, \"text\"].split()) for sample_id in range(NUM_SAMPLES)]\n",
    "assert LIST_NUM_WORDS == [10, 20, 20, 30, 50, 100]         # number of words in each sample needs to be consistent with competition requirements\n",
    "\n",
    "LIST_WORD_TO_ID: list[dict[str, int]] = [                  # map words to IDs for pretraining the pruning nn later\n",
    "    {word: i for i, word in enumerate(sorted(set(DF_SAMPLE.loc[sample_id, \"text\"].split())))}\n",
    "    for sample_id in range(NUM_SAMPLES)\n",
    "]\n",
    "\n",
    "assert LIST_WORD_TO_ID[5] == {'advent': 0, 'and': 1, 'angel': 2, 'as': 3, 'bake': 4, 'beard': 5, 'believe': 6, 'bow': 7, 'candle': 8, 'candy': 9, 'card': 10, 'carol': 11, 'cheer': 12, 'chimney': 13, 'chocolate': 14, 'cookie': 15, 'decorations': 16, 'doll': 17, 'dream': 18, 'drive': 19, 'eat': 20, 'eggnog': 21, 'elf': 22, 'family': 23, 'fireplace': 24, 'from': 25, 'fruitcake': 26, 'game': 27, 'gifts': 28, 'gingerbread': 29, 'give': 30, 'greeting': 31, 'grinch': 32, 'have': 33, 'hohoho': 34, 'holiday': 35, 'holly': 36, 'hope': 37, 'in': 38, 'is': 39, 'it': 40, 'jingle': 41, 'joy': 42, 'jump': 43, 'kaggle': 44, 'laugh': 45, 'magi': 46, 'merry': 47, 'milk': 48, 'mistletoe': 49, 'naughty': 50, 'nice': 51, 'night': 52, 'not': 53, 'nutcracker': 54, 'of': 55, 'ornament': 56, 'paper': 57, 'peace': 58, 'peppermint': 59, 'poinsettia': 60, 'polar': 61, 'puzzle': 62, 'reindeer': 63, 'relax': 64, 'scrooge': 65, 'season': 66, 'sing': 67, 'sleep': 68, 'sleigh': 69, 'snowglobe': 70, 'star': 71, 'stocking': 72, 'that': 73, 'the': 74, 'to': 75, 'toy': 76, 'unwrap': 77, 'visit': 78, 'walk': 79, 'we': 80, 'wish': 81, 'with': 82, 'wonder': 83, 'workshop': 84, 'wrapping': 85, 'wreath': 86, 'you': 87, 'yuletide': 88}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Main scoring module that computes perplexity scores from input texts. Used for all loss and optimization metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "class PerplexityCalculator:\n",
    "    \"\"\"\n",
    "    Main scoring module. Calculates the perplexity score of text(s) from a language model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str\n",
    "        Path to a pre-trained language model\n",
    "    load_in_8bit : bool, default=False\n",
    "        Use 8-bit quantization for the model. Requires CUDA.\n",
    "    device_map : str, default=\"auto\"\n",
    "        Device mapping for the language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        load_in_8bit: bool = False,\n",
    "        device_map: str = \"auto\",\n",
    "    ):\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "            model_path, padding_side=\"right\"\n",
    "        )\n",
    "        # Configure model loading based on quantization setting and device availability\n",
    "        if load_in_8bit:\n",
    "            if DEVICE.type != \"cuda\":\n",
    "                raise ValueError(\"8-bit quantization requires CUDA device\")\n",
    "            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "        else:\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16 if DEVICE.type == \"cuda\" else torch.float32,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    # if you load all batches on the gpu, the algo will run much faster than if any parts are offloaded to cpu\n",
    "    # so adjust batch_size accordingly to your how much vram you have\n",
    "    def get_score(self, input_texts: Union[str, List[str]], batch_size=10) -> Union[float, List[float]]:\n",
    "        single_input = isinstance(input_texts, str)\n",
    "        input_texts = [input_texts] if single_input else input_texts\n",
    "\n",
    "        loss_list = []\n",
    "\n",
    "        batches = len(input_texts) // batch_size + (len(input_texts) % batch_size != 0)\n",
    "        for j in range(batches):\n",
    "            a = j * batch_size\n",
    "            b = (j + 1) * batch_size\n",
    "            input_batch = input_texts[a:b]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Explicitly add sequence boundary tokens to the text\n",
    "                text_with_special = [\n",
    "                    f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n",
    "                    for text in input_batch\n",
    "                ]\n",
    "\n",
    "                # Tokenize\n",
    "                model_inputs = self.tokenizer(\n",
    "                    text_with_special,\n",
    "                    return_tensors=\"pt\",\n",
    "                    add_special_tokens=False,\n",
    "                    padding=True,\n",
    "                )\n",
    "\n",
    "                if \"token_type_ids\" in model_inputs:\n",
    "                    model_inputs.pop(\"token_type_ids\")\n",
    "\n",
    "                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
    "\n",
    "                # Get model output\n",
    "                output = self.model(**model_inputs, use_cache=False)\n",
    "                logits = output[\"logits\"]\n",
    "\n",
    "                label = model_inputs[\"input_ids\"]\n",
    "                label[label == self.tokenizer.pad_token_id] = PAD_TOKEN_LABEL_ID\n",
    "\n",
    "                # Shift logits and labels for calculating loss\n",
    "                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n",
    "                shift_labels = label[..., 1:].contiguous()  # Drop first input\n",
    "\n",
    "                # Calculate token-wise loss\n",
    "                loss = self.loss_fct(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "                )\n",
    "\n",
    "                loss = loss.view(len(logits), -1)\n",
    "                valid_length = (shift_labels != PAD_TOKEN_LABEL_ID).sum(dim=-1)\n",
    "                loss = torch.sum(loss, -1) / valid_length\n",
    "\n",
    "                loss_list += loss.cpu().tolist()\n",
    "\n",
    "        ppl = [math.exp(i) for i in loss_list]\n",
    "\n",
    "        return ppl[0] if single_input else ppl\n",
    "\n",
    "    def clear_gpu_memory(self) -> None:\n",
    "        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "\n",
    "        # Delete model and tokenizer if they exist\n",
    "        if hasattr(self, \"model\"):\n",
    "            del self.model\n",
    "        if hasattr(self, \"tokenizer\"):\n",
    "            del self.tokenizer\n",
    "\n",
    "        # Run garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        # Clear CUDA cache and reset memory stats\n",
    "        with DEVICE:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at what the data provided by the competition looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>advent chimney elf family fireplace gingerbrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>advent chimney elf family fireplace gingerbrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>yuletide decorations gifts cheer holiday carol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>yuletide decorations gifts cheer holiday carol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>hohoho candle poinsettia snowglobe peppermint ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text\n",
       "0   0  advent chimney elf family fireplace gingerbrea...\n",
       "1   1  advent chimney elf family fireplace gingerbrea...\n",
       "2   2  yuletide decorations gifts cheer holiday carol...\n",
       "3   3  yuletide decorations gifts cheer holiday carol...\n",
       "4   4  hohoho candle poinsettia snowglobe peppermint ..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = pd.read_csv('../input/sample_submission.csv')\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are 2 columns, one is 'id', the other is 'text'; let's take a look at all the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'text': 'advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge'}\n",
      "{'id': 1, 'text': 'advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge walk give jump drive bake the sleep night laugh and'}\n",
      "{'id': 2, 'text': 'yuletide decorations gifts cheer holiday carol magi nutcracker polar grinch sleigh chimney workshop stocking ornament holly jingle beard naughty nice'}\n",
      "{'id': 3, 'text': 'yuletide decorations gifts cheer holiday carol magi nutcracker polar grinch sleigh chimney workshop stocking ornament holly jingle beard naughty nice sing cheer and of the is eat visit relax unwrap'}\n",
      "{'id': 4, 'text': 'hohoho candle poinsettia snowglobe peppermint eggnog fruitcake chocolate candy puzzle game doll toy workshop wonder believe dream hope peace joy merry season greeting card wrapping paper bow fireplace night cookie milk star wish wreath angel the to of and in that have it not with as you from we kaggle'}\n",
      "{'id': 5, 'text': 'advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge walk give jump drive bake the sleep night laugh and yuletide decorations gifts cheer holiday carol magi nutcracker polar grinch sleigh chimney workshop stocking ornament holly jingle beard naughty nice sing cheer and of the is eat visit relax unwrap hohoho candle poinsettia snowglobe peppermint eggnog fruitcake chocolate candy puzzle game doll toy workshop wonder believe dream hope peace joy merry season greeting card wrapping paper bow fireplace night cookie milk star wish wreath angel the to of and in that have it not with as you from we kaggle'}\n"
     ]
    }
   ],
   "source": [
    "for _, row in sample_df.iterrows():\n",
    "    print(row.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks like we have 6 strings of texts, made up with holiday-themed vocabularies separated by whitespaces.\n",
    "\n",
    "- According to the rules of the competition, our goal is to rearrange the words in a way that results in:\n",
    "    - the **lowest perplexity score** as evaluated by an LLM.\n",
    "    - we'll use `Gemma-2 9b` as recommended by the competition.\n",
    "- Perplexity bascially means how likely the LLM expect its next token to be.\n",
    "    - if a token has low perplexity, it means the LLM is likely to sample it as the next token in sequence.\n",
    "    - else if a token has high perplexity, it mean the LLM is unlikely to sample it as the next token in sequence.\n",
    "- The final leaderboard score is the **average perplexity score** across all 6 sample strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brainstorm Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since the goal is to find a specific permutation for each string, that results in the lowest perplexity score, can we just brute force all permutations to find the best score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_4e3d0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_4e3d0_level0_col0\" class=\"col_heading level0 col0\" >id</th>\n",
       "      <th id=\"T_4e3d0_level0_col1\" class=\"col_heading level0 col1\" >word_count</th>\n",
       "      <th id=\"T_4e3d0_level0_col2\" class=\"col_heading level0 col2\" >permutations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_4e3d0_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_4e3d0_row0_col1\" class=\"data row0 col1\" >10</td>\n",
       "      <td id=\"T_4e3d0_row0_col2\" class=\"data row0 col2\" >3.63e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4e3d0_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_4e3d0_row1_col1\" class=\"data row1 col1\" >20</td>\n",
       "      <td id=\"T_4e3d0_row1_col2\" class=\"data row1 col2\" >2.43e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4e3d0_row2_col0\" class=\"data row2 col0\" >2</td>\n",
       "      <td id=\"T_4e3d0_row2_col1\" class=\"data row2 col1\" >20</td>\n",
       "      <td id=\"T_4e3d0_row2_col2\" class=\"data row2 col2\" >2.43e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4e3d0_row3_col0\" class=\"data row3 col0\" >3</td>\n",
       "      <td id=\"T_4e3d0_row3_col1\" class=\"data row3 col1\" >30</td>\n",
       "      <td id=\"T_4e3d0_row3_col2\" class=\"data row3 col2\" >2.65e+32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4e3d0_row4_col0\" class=\"data row4 col0\" >4</td>\n",
       "      <td id=\"T_4e3d0_row4_col1\" class=\"data row4 col1\" >50</td>\n",
       "      <td id=\"T_4e3d0_row4_col2\" class=\"data row4 col2\" >3.04e+64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_4e3d0_row5_col0\" class=\"data row5 col0\" >5</td>\n",
       "      <td id=\"T_4e3d0_row5_col1\" class=\"data row5 col1\" >100</td>\n",
       "      <td id=\"T_4e3d0_row5_col2\" class=\"data row5 col2\" >9.33e+157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x24b5ff77170>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df[\"word_count\"] = sample_df[\"text\"].str.split().str.len()\n",
    "sample_df[\"permutations\"] = sample_df[\"word_count\"].apply(math.factorial).apply(lambda x: f\"{x:.2e}\")\n",
    "sample_df[[\"id\", \"word_count\", \"permutations\"]].style.hide(axis=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, only the 1st sample `sample_0`, with a word count of 10, with a total number of about 3.6 million permutations, is feasible for brute-force.\n",
    "- To brute-force the other strings would take until the end of the universe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then, since the competition's evalutation metric is perplexity, and perplexity for `language models` should mean that the better the text flows according to natural language, the better the perplexity, right? Let's do some experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf42d4e170f445d0a1c55e7b69bc16ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1st, instantiate the scorer\n",
    "scorer = PerplexityCalculator(model_path=str(PATH_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_0='advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge'\n",
      "\n",
      "sample 0 perplexity score: 3857.64626282737\n"
     ]
    }
   ],
   "source": [
    "# test if scorer works\n",
    "sample_0 = sample_df.text[0]\n",
    "print(f\"{sample_0=}\")\n",
    "score = scorer.get_score(sample_0)\n",
    "print()\n",
    "print(f\"sample 0 perplexity score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The scorer works correctly, currently, the default arrangement of sample 0 scored about `3857.64`\n",
    "- Next, let's see what the longest sample, `sample 5` scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_5='advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge walk give jump drive bake the sleep night laugh and yuletide decorations gifts cheer holiday carol magi nutcracker polar grinch sleigh chimney workshop stocking ornament holly jingle beard naughty nice sing cheer and of the is eat visit relax unwrap hohoho candle poinsettia snowglobe peppermint eggnog fruitcake chocolate candy puzzle game doll toy workshop wonder believe dream hope peace joy merry season greeting card wrapping paper bow fireplace night cookie milk star wish wreath angel the to of and in that have it not with as you from we kaggle'\n",
      "\n",
      "sample 5 perplexity score: 354.636652059297\n"
     ]
    }
   ],
   "source": [
    "sample_5 = sample_df.text[5]\n",
    "print(f\"{sample_5=}\")\n",
    "score = scorer.get_score(sample_5)\n",
    "print()\n",
    "print(f\"sample 5 perplexity score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Default sample 5 scored much better than sample 1 due to it having many more \"stopwords\" available.\n",
    "- Such as *\"the, of, and, that\" etc.*\n",
    "- Now, what if we put all these stopwords at the start of the sentence? you would think that it would score worse, since the sentence would not make sense anymore, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the and and of the is the to of and in that have it not with as you from we advent chimney elf family fireplace gingerbread mistletoe ornament reindeer scrooge walk give jump drive bake sleep night laugh yuletide decorations gifts cheer holiday carol magi nutcracker polar grinch sleigh chimney workshop stocking ornament holly jingle beard naughty nice sing cheer eat visit relax unwrap hohoho candle poinsettia snowglobe peppermint eggnog fruitcake chocolate candy puzzle game doll toy workshop wonder believe dream hope peace joy merry season greeting card wrapping paper bow fireplace night cookie milk star wish wreath angel kaggle\n",
      "\n",
      "new sample 5 perplexity score: 233.4866728754002\n"
     ]
    }
   ],
   "source": [
    "def rearrange_words(sentence, stopwords):\n",
    "    words = sentence.split()\n",
    "    stop_words = [word for word in words if word in stopwords]\n",
    "    other_words = [word for word in words if word not in stopwords]\n",
    "    return \" \".join(stop_words + other_words)\n",
    "\n",
    "stopwords = \"and as from have in is it not of that the to we with you\"\n",
    "sorted_sample_5 = rearrange_words(sample_5, stopwords)\n",
    "print(sorted_sample_5)\n",
    "\n",
    "score = scorer.get_score(sorted_sample_5)\n",
    "print()\n",
    "print(f\"new sample 5 perplexity score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- suprise, sorted sample 5 actually scores much better, even with a non-grammatical arrangement like this.\n",
    "- this is likely due to the 'quirks' of how LLM progenate their embeddings layers.\n",
    "- so, if for the purpose of receiving a better perplexity score, focusing solely on grammar and semantics will not do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So, it looks like we'll need to employ optimization algorithms to find the permutations we need; there are many optimization algorithms, such as simulated annealing, genetic algorithm etc. As usual, the devil is in the (implementation) details, below is an example algorithm called `Iterated Local Search`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterated Local Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The core idea of the **iterated local search** is that\n",
    "    - we perform **local search** and greedily accept better solutions:\n",
    "```pseudocode\n",
    "FUNCTION solve(initial_perm)\n",
    "    best_perm ← initial_perm\n",
    "    LOOP FOREVER:\n",
    "        perms ← best_perm\n",
    "        REPEAT n TIMES:\n",
    "            perms ← kick(perms)\n",
    "            perms ← local_search(perms)\n",
    "            IF score(perms) < score(best_perm) THEN\n",
    "                best_perm ← perms\n",
    "\n",
    "```\n",
    "- In **local search**, we:\n",
    "    - using depth-first-search, we continue to search as long as a better solution can be found from this `initial_perm`\n",
    "```pseudocode\n",
    "FUNCTION local_search(initial_perm)\n",
    "    perms ← initial_perm\n",
    "    LOOP FOREVER:\n",
    "        result ← DFS(perms)\n",
    "        IF result IS NULL THEN\n",
    "            RETURN perms\n",
    "        perms ← result \n",
    "```\n",
    "\n",
    "- In **DFS**, we:\n",
    "    - immediately return if a better solution is found\n",
    "    - else, accept solutions with a worse score, as allowed by a `threshold` parameter\n",
    "    - and recursively search until a valid result or no result can be found\n",
    "\n",
    "```pseudocode\n",
    "FUNCTION DFS(best_score, perms, depth)\n",
    "    FOR EACH neighbor IN make_neighbors(perms) DO:\n",
    "        IF score(neighbor) < score(perms) THEN\n",
    "            RETURN neighbor\n",
    "        ELSE IF score(neighbor) < get_threshold(best_score, depth) THEN\n",
    "            result ← DFS(best_score, neighbor, depth + 1)\n",
    "            IF result IS NOT NULL THEN\n",
    "                RETURN result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The following codes are the implementations for the above pseudocodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varous utility functions to aid the run. Refer to the docstrings for the purposes of the util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_words_best(n_idx):\n",
    "    \"\"\"\n",
    "    Retrieve the permutation with the best score for a sample.\n",
    "\n",
    "    Searches the saved text and returns the best score and corresponding words.\n",
    "    Assert the permutation's validity by comparing its sorted words to the original text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_idx : int\n",
    "        The sample index to retrieve.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[float, list[str]] or (None, None)\n",
    "        The best score and permutation if available; otherwise, (None, None).\n",
    "    \"\"\"\n",
    "    path_save_idx = PATH_SAVE / f\"{n_idx:04d}\"\n",
    "    words_original = DF_SAMPLE.loc[n_idx, \"text\"].split(\" \")\n",
    "    path_txt = path_save_idx.glob(\"*.txt\")\n",
    "    list_path_txt = list(path_txt)\n",
    "\n",
    "    if not list_path_txt:\n",
    "        return None, None\n",
    "    \n",
    "    list_scores = [float(path.stem.split(\"_\")[0]) for path in list_path_txt]\n",
    "    idx_min = np.argmin(list_scores)\n",
    "    score = list_scores[idx_min]\n",
    "    path_min = list_path_txt[idx_min]\n",
    "    text_min = path_min.read_text()\n",
    "    words_min = text_min.split(\" \")\n",
    "    \n",
    "    assert sorted(words_min) == sorted(words_original)\n",
    "    return score, words_min\n",
    "\n",
    "\n",
    "def save_text(get_score, n_idx, text, verbose=0):\n",
    "    \"\"\"\n",
    "    Save a permutation along with its score to disk.\n",
    "\n",
    "    Again validates that the submitted text is a valid permutation of the original,\n",
    "    calculates its score, and saves the text with the score and a hash-based filename.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    get_score : function\n",
    "        The scoring function\n",
    "    n_idx : int\n",
    "        The sample id\n",
    "    text : str\n",
    "        The permutation to be saved.\n",
    "    verbose : int, optional\n",
    "        Verbosity: prints score if >=1 and text if >=2, default 0.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float or None\n",
    "        The calculated score if saving is successful; otherwise, None.\n",
    "    \"\"\"\n",
    "    path_save_idx = PATH_SAVE / f\"{n_idx:04d}\"\n",
    "    path_save_idx.mkdir(exist_ok=True)\n",
    "    text_original = DF_SAMPLE.loc[n_idx, \"text\"]\n",
    "    words_original = text_original.split(\" \")\n",
    "    words = text.split(\" \")\n",
    "    \n",
    "    if sorted(words) != sorted(words_original):\n",
    "        print(f\"[Warning] words are not the same with original: {words} != {words_original}\")\n",
    "        return\n",
    "    \n",
    "    text = \" \".join(words)\n",
    "    score = get_score(n_idx, text)\n",
    "    \n",
    "    if verbose >= 1: print(f\"score:{score:.4f}\")\n",
    "    if verbose >= 2: print(text)\n",
    "    \n",
    "    md5 = hashlib.md5(text.encode()).hexdigest()\n",
    "    path_save_text = path_save_idx / f\"{score:.4f}_{md5}.txt\"\n",
    "    \n",
    "    with path_save_text.open(\"w\") as f:\n",
    "        f.write(text)\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def load_score_memo() -> tuple[dict[str, float], dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Load score memoization from disk.\n",
    "\n",
    "    Retrieves two dictionaries: successful scores and errored scores.\n",
    "    These memos avoid recomputation during optimization run.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[dict[str, float], dict[str, float]]\n",
    "        A tuple containing the successful score memo and errored score memo.\n",
    "    \"\"\"\n",
    "    def load(name: str) -> dict[str, float]:\n",
    "        path_score_memo = PATH_SAVE / name\n",
    "        if path_score_memo.exists():\n",
    "            with path_score_memo.open(\"rb\") as f:\n",
    "                return pickle.load(f)\n",
    "        return {}\n",
    "    \n",
    "    return load(\"score_memo.pkl\"), load(\"score_memo_with_error.pkl\")\n",
    "\n",
    "\n",
    "def save_score_memo(\n",
    "    score_memo: dict[str, float],\n",
    "    score_memo_with_error: dict[str, float],\n",
    "):\n",
    "    \"\"\"\n",
    "    Save updated score memoization dictionaries to disk.\n",
    "\n",
    "    Merges new score memo entries with existing ones and writes the updated\n",
    "    dictionaries back to disk to enable caching during the optimization run.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    score_memo : dict[str, float]\n",
    "        Dictionary mapping text strings to their scores.\n",
    "    score_memo_with_error : dict[str, float]\n",
    "        Dictionary mapping text strings to their scores, including error cases.\n",
    "    \"\"\"\n",
    "    def save(name: str, score_memo: dict[str, float]):\n",
    "        path_score_memo = PATH_SAVE / name\n",
    "        with path_score_memo.open(\"wb\") as f:\n",
    "            pickle.dump(score_memo, f)\n",
    "    \n",
    "    score_memo_original, score_memo_with_error_original = load_score_memo()\n",
    "    score_memo_original.update(score_memo)\n",
    "    score_memo_with_error_original.update(score_memo_with_error)\n",
    "    \n",
    "    save(\"score_memo.pkl\", score_memo_original)\n",
    "    save(\"score_memo_with_error.pkl\", score_memo_with_error_original)\n",
    "\n",
    "\n",
    "def _get_score(\n",
    "    scorer,\n",
    "    score_memo: dict[str, float],\n",
    "    score_memo_with_error: dict[str, float],\n",
    "    text: Union[str, List[str]],\n",
    ") -> Union[float, List[float]]:\n",
    "    \"\"\"\n",
    "    Get the perplexity score(s) for input text(s), use memoization to avoid recomputation.\n",
    "\n",
    "    Checks if the score for the given text or texts is already cached in the\n",
    "    score memos. If not, computes the score using the provided scorer, updates\n",
    "    the memo, and returns the score(s).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scorer : PerplexityCalculator\n",
    "        An instance that can compute the score for text.\n",
    "    score_memo : dict[str, float]\n",
    "        Cache of previously computed scores.\n",
    "    score_memo_with_error : dict[str, float]\n",
    "        Cache for scores computed when errors occurred.\n",
    "    text : str or list[str]\n",
    "        A single text string or a list of text strings for which to compute perplexity.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float or list[float]\n",
    "        The perplexity score if a single string is provided; otherwise, return a list of scores.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If `text` is neither a string nor a list of strings.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        if text in score_memo:\n",
    "            return score_memo[text]\n",
    "        \n",
    "        score = scorer.get_score(text)\n",
    "        score_memo[text] = score\n",
    "        return score\n",
    "    \n",
    "    elif isinstance(text, list):\n",
    "        list_text_new = [t for t in text if t not in score_memo and t not in score_memo_with_error]\n",
    "        \n",
    "        if list_text_new:\n",
    "            list_score_new = scorer.get_score(list_text_new)\n",
    "            for t, s in zip(list_text_new, list_score_new):\n",
    "                score_memo_with_error[t] = s\n",
    "        \n",
    "        return [score_memo.get(t, score_memo_with_error.get(t, None)) for t in text]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"text is not str nor list[str]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train Pruning NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this module we train a lightweight CNN, to be used for pruning permutation candidates generated.\n",
    "- So we save on scoring compute cost when doing optimization runs.\n",
    "- we will need a score memo saved first to use as training data.\n",
    "- I will not run the training here in this notebook, but I've included the training results in the main run below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation block. *arXiv:1709.01507*\n",
    "\n",
    "    This block adaptively recalibrates channel-wise feature responses by explicitly modeling\n",
    "    interdependencies between channels. It performs global average pooling followed by two\n",
    "    fully connected layers with a ReLU and sigmoid activation to compute per-channel weights.\n",
    "\n",
    "    tldr: emphasize important feature channels\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, reduction: int = 16) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        channels : int\n",
    "            Number of input channels.\n",
    "        reduction : int, optional\n",
    "            Reduction factor for the hidden layer in the SE block, default 16.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
    "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (B, C, L).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor after channel-wise recalibration.\n",
    "        \"\"\"\n",
    "        b, c, l = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.sigmoid(y).view(b, c, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with two convolutional layers and an SEBlock.\n",
    "    Implements a residual connection where the input is added to the processed features.\n",
    "    \n",
    "    tldr: helps in training deeper networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, kernel_size: int = 3) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        channels : int\n",
    "            Number of input and output channels.\n",
    "        kernel_size : int, optional\n",
    "            Convolutional kernel size, default 3.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv1d(channels, channels, kernel_size, padding=padding)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, kernel_size, padding=padding)\n",
    "        self.se = SEBlock(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (B, channels, L).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor after applying convolutional layers, SE block, and residual addition.\n",
    "        \"\"\"\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.se(out)\n",
    "        out += x\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PruneNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Pretraining model architecture: embedding layer, convolutional stem, ResidualBlocks,\n",
    "    and final fully connected layer to output a single scalar per input sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, channels: int, num_blocks: int) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab_size : int\n",
    "            Size of the vocabulary.\n",
    "        channels : int\n",
    "            Number of channels for the embeddings and convolutional layers.\n",
    "        num_blocks : int\n",
    "            Number of ResidualBlocks to stack.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, channels)\n",
    "        self.conv_stem = nn.Conv1d(\n",
    "            channels, channels, kernel_size=7, stride=1, padding=3, bias=False\n",
    "        )\n",
    "        self.relu_stem = nn.ReLU(inplace=True)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [ResidualBlock(channels) for _ in range(num_blocks)]\n",
    "        )\n",
    "        self.fc = nn.Linear(channels, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (B, L) containing token indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor of shape (B, 1) representing a scalar score for each sequence.\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)                        # (B, L, channels)\n",
    "        x = x.transpose(1, 2)                        # (B, channels, L)\n",
    "        x = self.conv_stem(x)\n",
    "        x = self.relu_stem(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = F.adaptive_avg_pool1d(x, 1).squeeze(-1)  # (B, channels)\n",
    "        x = self.fc(x)                               # (B, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScoreDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for pretraining, holding input text sequences and their target scores.\n",
    "\n",
    "    This dataset stores the input token tensors and their corresponding target scores (log values).\n",
    "    \"\"\"\n",
    "    def __init__(self, X: torch.Tensor, y: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : torch.Tensor\n",
    "            Tensor of input sequences (token IDs) of shape (N, L).\n",
    "        y : torch.Tensor\n",
    "            Tensor of target scores (log values) of shape (N,).\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Retrieve the input and target score for the given index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of the sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[torch.Tensor, torch.Tensor]\n",
    "            A tuple (X, y) where X is the token tensor and y is the target score.\n",
    "        \"\"\"\n",
    "        X = self.X[idx].long()\n",
    "        y = self.y[idx]\n",
    "        return X, y\n",
    "\n",
    "\n",
    "def prepare_dataset(training_data: list[dict[bytes, float]], sample_id: int) -> tuple[dict[str, int], Dataset]:\n",
    "    \"\"\"\n",
    "    Prepare the training dataset for a specific sample.\n",
    "\n",
    "    Converts the compressed text data into a tensor of token IDs and corresponding log scores,\n",
    "    and returns the word-to-ID mapping along with the constructed PyTorch Dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    training_data : list[dict[bytes, float]]\n",
    "        A list where each element corresponds to a sample and maps compressed text (bytes)\n",
    "        to its associated score.\n",
    "    sample_id : int\n",
    "        The index of the sample to prepare the dataset for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[dict[str, int], Dataset]\n",
    "        A tuple containing the word-to-ID mapping and the constructed ScoreDataset.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        When input is sample_id 1 or 2.\n",
    "    \"\"\"\n",
    "    word_to_id = LIST_WORD_TO_ID[sample_id]\n",
    "    length = LIST_NUM_WORDS[sample_id]\n",
    "\n",
    "    if sample_id in [1, 2]:\n",
    "        raise ValueError(\"sample_id 1 and 2 are not supported\")\n",
    "    \n",
    "    num_data = len(training_data[sample_id])\n",
    "    X = torch.empty((num_data, length), dtype=torch.int8)\n",
    "    y = torch.empty(num_data, dtype=torch.float)\n",
    "\n",
    "    for idx, (compressed_text, score) in enumerate(tqdm(training_data[sample_id].items(), mininterval=30)):\n",
    "        assert len(compressed_text) == length\n",
    "        X[idx] = torch.tensor(list(compressed_text), dtype=torch.int8)\n",
    "        y[idx] = math.log(score)\n",
    "\n",
    "    # print(f\"[DEBUG] {word_to_id=}\")\n",
    "    dataset = ScoreDataset(X, y)\n",
    "    return word_to_id, dataset\n",
    "\n",
    "\n",
    "def train_model(sample_id: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Train the model on pretraining data for a given sample.\n",
    "\n",
    "    Loads training data, splits it into training and validation sets, and trains the model\n",
    "    using L1 loss on log-transformed scores. Save model checkpoints after each epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_id : int, optional\n",
    "        The sample index to train on (default 5th sample).\n",
    "    \"\"\"\n",
    "    _, training_data = load_score_memo()\n",
    "    path_pretrain = PATH_SAVE / Path(\"pretrain\")\n",
    "    path_pretrain.mkdir(parents=True, exist_ok=True)\n",
    "    num_epochs = 20\n",
    "    batch_size = 4096\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    word_to_id, dataset = prepare_dataset(training_data, sample_id)\n",
    "    del training_data\n",
    "\n",
    "    total_size = len(dataset)\n",
    "    val_size = int(total_size * 0.05)        # split 5% data as validation\n",
    "    train_size = total_size - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4, shuffle=False)\n",
    "    model = PruneNet(vocab_size=len(word_to_id), channels=128, num_blocks=12)\n",
    "    model = model.to(device)\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.005)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\", mininterval=30)\n",
    "        for X, y in pbar:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X)  # (B, 1)\n",
    "            loss = F.l1_loss(outputs.squeeze(-1), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * X.size(0)\n",
    "            count += X.size(0)\n",
    "            pbar.set_postfix(refresh=False, loss=loss.item())\n",
    "\n",
    "        avg_loss = running_loss / count\n",
    "\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                preds: torch.Tensor = model(X).squeeze(-1)\n",
    "                all_preds.extend(preds.detach().cpu().tolist())\n",
    "                all_targets.extend(y.detach().cpu().tolist())\n",
    "        corr, _ = spearmanr(all_targets, all_preds)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, \"\n",
    "            f\"Val Spearman: {corr:.4f}\"\n",
    "        )\n",
    "\n",
    "        torch.save(\n",
    "            {\"model\": model.state_dict(), \"word_to_id\": word_to_id},\n",
    "            path_pretrain / f\"model_{sample_id}_epoch_{epoch+1}.pt\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run if previous score memos are saved, to be used as training data for the nn\n",
    "# train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is the main search loop module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the pretrained pruning neural network to estimate scores for candiate permutations. \n",
    "    - So we can prune potential candidates and don't have to fully score everything through the LLM, which is expensive.\n",
    "- The estimator will try to load any model checkpoints generated during the optimization run first, else fallback to the pretrained checkpoints.\n",
    "- During optimization run, the nn's weights will be updated on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreEstimator:\n",
    "    \"\"\"\n",
    "    Use the pretrained pruning neural network to estimate scores for candiate permutations,\n",
    "    so we don't have to pass every candidate into the full LM to score, which is expensive.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_id: int,\n",
    "        epoch: int,\n",
    "        device: torch.device,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        sample_id : int\n",
    "            Index of the sample for which the estimator is used on.\n",
    "        epoch : int\n",
    "            Epoch number of the pretrained model (used when no online model exists).\n",
    "        device : torch.device\n",
    "            The device (CPU or CUDA) on which the model will run.\n",
    "        \"\"\"\n",
    "        self.sample_id = sample_id\n",
    "        self.length = LIST_NUM_WORDS[sample_id]\n",
    "\n",
    "        # Prepare the data for the online model.\n",
    "        online_model_dir = PATH_SAVE / \"online\"\n",
    "        online_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.online_model_path = online_model_dir / f\"model_{sample_id}.pt\"\n",
    "        self.pretrained_model_path = (PATH_SAVE / f\"pretrain/model_{sample_id}_epoch_{epoch}.pt\")\n",
    "        \n",
    "        # Load an existing online model if available, otherwise fall back to the pretrained model.\n",
    "        if self.online_model_path.exists():\n",
    "            print(f\"[ScoreEstimator] Load online model: {self.online_model_path}\")\n",
    "            checkpoint = torch.load(self.online_model_path, map_location=device, weights_only=True)\n",
    "        elif self.pretrained_model_path.exists():\n",
    "            print(f\"[ScoreEstimator] Load pretrained model: {self.pretrained_model_path}\")\n",
    "            checkpoint = torch.load(self.pretrained_model_path, map_location=device, weights_only=True)\n",
    "        else:\n",
    "            # warnings.warn(f\"[ScoreEstimator] Checkpoint not found: {sample_id}\")\n",
    "            checkpoint = None\n",
    "\n",
    "        # Retrieve word-to-ID mapping for the sample.\n",
    "        self.word_to_id = LIST_WORD_TO_ID[sample_id]\n",
    "\n",
    "        self.device = device\n",
    "        self.model = PruneNet(vocab_size=len(self.word_to_id), channels=128, num_blocks=12).to(device)\n",
    "        if checkpoint is not None:\n",
    "            self.model.load_state_dict(checkpoint[\"model\"])\n",
    "        self.model.eval()  # remember to set to eval() to lock weights\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.00005) # tune eta here\n",
    "\n",
    "        # For debugging, buffers to collect texts, scores, and predictions.\n",
    "        self.buffer_texts = []\n",
    "        self.buffer_scores = []\n",
    "        self.buffer_predictions = []\n",
    "        self.update_count = 0\n",
    "\n",
    "\n",
    "    def estimate_scores(self, texts: list[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Converts each text into a sequence of token IDs using the word-to-ID mapping,\n",
    "        then computes the predicted scores for the optimization run, and for online training.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        texts : list[str]\n",
    "            A list of text strings representing candidate solutions.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            An array of log perplexity scores (one per text).\n",
    "        \"\"\"\n",
    "        X_list = []\n",
    "\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            assert len(words) == self.length\n",
    "            X_list.append([self.word_to_id[w] for w in words])\n",
    "\n",
    "        X = torch.tensor(X_list, dtype=torch.long, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            preds: torch.Tensor = self.model(X).squeeze(-1)  # (B,)\n",
    "\n",
    "        return preds.detach().cpu().numpy()                  # log perplexity\n",
    "\n",
    "\n",
    "    def update_parameters(self, texts: list[str], scores: list[float]):\n",
    "        \"\"\"\n",
    "        Update the model weights using the data and scores obtained during the optimization run.\n",
    "        This is the \"online training\" aspect.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        texts : list[str]\n",
    "            A list of text strings representing candidate solutions.\n",
    "        scores : list[float]\n",
    "            A list of target scores corresponding to the texts.\n",
    "        \"\"\"\n",
    "        X_list = []\n",
    "\n",
    "        for text in texts:\n",
    "            words = text.split()\n",
    "            assert len(words) == self.length\n",
    "            X_list.append([self.word_to_id[w] for w in words])\n",
    "\n",
    "        X = torch.tensor(X_list, dtype=torch.long, device=self.device)\n",
    "        self.model.train()\n",
    "        pred: torch.Tensor = self.model(X).squeeze(-1)  # (B,)\n",
    "        target = torch.tensor(scores, dtype=torch.float, device=self.device).log()\n",
    "        loss = F.l1_loss(pred, target)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "        self.model.eval()\n",
    "\n",
    "        self.buffer_texts.extend(texts)\n",
    "        self.buffer_scores.extend(scores)\n",
    "        self.buffer_predictions.extend(pred.detach().cpu().tolist())\n",
    "        self.update_count += 1\n",
    "        \n",
    "        if self.update_count % 256 == 0:\n",
    "            corr, _ = spearmanr(self.buffer_scores, self.buffer_predictions)\n",
    "            print(f\"[ScoreEstimator] Spearman: {corr:.4f}\")\n",
    "            self.buffer_texts.clear()\n",
    "            self.buffer_scores.clear()\n",
    "            self.buffer_predictions.clear()\n",
    "\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(\n",
    "            {\"word_to_id\": self.word_to_id, \n",
    "             \"model\": self.model.state_dict()},\n",
    "             self.online_model_path,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using 2 types of moves to create new perms to continue the optimization run.\n",
    "    - Move A: Inserting into Sorted Segments.\n",
    "    - Move B: Random Center-Based Shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_neighbors(words: list[str]) -> Generator[tuple[list[str], tuple], None, None]:\n",
    "    \"\"\"\n",
    "    Generate neighboring permutations for a given list of words.\n",
    "\n",
    "    Identifies sorted segments in the list, then:\n",
    "    1. Merge parts of the candidate with parts of the sorted segments.\n",
    "    2. Moving a block of words to different positions.\n",
    "\n",
    "    Tracks all seen permutations in a set to prevent duplicates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words : list[str]\n",
    "        The original sequence of words to perturb.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    tuple[list[str], tuple]\n",
    "        A tuple where the first element is a new permutation and the\n",
    "        second element is a tuple encoding metadata about the type of change.\n",
    "    \"\"\"\n",
    "    words = words.copy()\n",
    "    found = {tuple(words)}\n",
    "\n",
    "    # Get continous segments in the words that are already sorted. \n",
    "    sorted_segments = []\n",
    "    for i, (left_word, right_word) in enumerate(zip(words, words[1:])):\n",
    "        if left_word <= right_word:\n",
    "            if sorted_segments and sorted_segments[-1][1] == i + 1:\n",
    "                sorted_segments[-1][1] = i + 2\n",
    "            else:\n",
    "                sorted_segments.append([i, i + 2])\n",
    "\n",
    "    # Keep the sorted segments but only if >= 4 long.\n",
    "    sorted_segments = [(left, right) for left, right in sorted_segments if right - left >= 4]\n",
    "\n",
    "    # set max_length to 3, but decrease to 2 if it's sample 4 & 5, else too expensive.\n",
    "    max_length = 2 if len(words) >= 50 else 3\n",
    "    \n",
    "    # Move A: Inserting into Sorted Segments\n",
    "    # Loop Over Possible Segment Lengths\n",
    "    for length in range(1, max_length + 1):\n",
    "        if length >= 2:\n",
    "            results = []\n",
    "            # Loop Over Source Segments\n",
    "            for source_l in range(len(words) - length + 1):\n",
    "                source_r = source_l + length\n",
    "                # Loop Over Target Segments\n",
    "                for target_l, target_r in sorted_segments:\n",
    "                    if source_r <= target_l:\n",
    "                        # Insert the words to create perms\n",
    "                        permuted = (\n",
    "                            words[:source_l]\n",
    "                            + words[source_r:target_l]\n",
    "                            + sorted(words[source_l:source_r] + words[target_l:target_r])\n",
    "                            + words[target_r:]\n",
    "                        )\n",
    "                    elif target_r <= source_l:\n",
    "                        permuted = (\n",
    "                            words[:target_l]\n",
    "                            + sorted(words[target_l:target_r] + words[source_l:source_r])\n",
    "                            + words[target_r:source_l]\n",
    "                            + words[source_r:]\n",
    "                        )\n",
    "                    # continue if overlapping\n",
    "                    else:\n",
    "                        continue\n",
    "                    # make sure no duplicates\n",
    "                    if (t := tuple(permuted)) not in found:\n",
    "                        found.add(t)\n",
    "                        results.append((permuted, (source_l, source_r, target_l, target_r, 3)))\n",
    "\n",
    "            random.shuffle(results)\n",
    "            # return each result\n",
    "            yield from results\n",
    "\n",
    "        # Move B: Random Center-Based Shifts\n",
    "        # Valid centers\n",
    "        r = range(length, len(words) - length + 1) \n",
    "        for center in random.sample(r, len(r)):\n",
    "            results = []\n",
    "            # Right-centered\n",
    "            right = center + length\n",
    "            # Shift right to left\n",
    "            for left_length in itertools.count(length):\n",
    "                left = center - left_length\n",
    "                if left < 0:\n",
    "                    break\n",
    "                permuted = (\n",
    "                    words[:left]\n",
    "                    + words[center:right]\n",
    "                    + words[left:center]\n",
    "                    + words[right:]\n",
    "                )\n",
    "                if (t := tuple(permuted)) not in found:\n",
    "                    found.add(t)\n",
    "                    results.append((permuted, (left, center, right, 0)))\n",
    "            \n",
    "            # Left-centered\n",
    "            left = center - length\n",
    "            # Shift left to right\n",
    "            for right_length in itertools.count(length + 1):\n",
    "                right = center + right_length\n",
    "                if right > len(words):\n",
    "                    break\n",
    "                permuted = (\n",
    "                    words[:left]\n",
    "                    + words[center:right]\n",
    "                    + words[left:center]\n",
    "                    + words[right:]\n",
    "                )\n",
    "                if (t := tuple(permuted)) not in found:\n",
    "                    found.add(t)\n",
    "                    results.append((permuted, (left, center, right, 0)))\n",
    "\n",
    "            random.shuffle(results)\n",
    "            yield from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This is the main optimization implementation that controls all the search mechanisms.\n",
    "\n",
    "- the entry point is the `run()` method.\n",
    "- during each iteration of `run()`, `hillclimbing()` method is called that implemented the local search logic.\n",
    "- if the search is stuck, a random perturbation is applied to try to jostle the run out of the local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization:\n",
    "    \"\"\"\n",
    "    Orchestrates the entire optimization process for all samples.\n",
    "        - local search / hill climbing\n",
    "        - perturbations / kicks\n",
    "        - neural network online update\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        flag_use_best=True,\n",
    "        flag_shuffle=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        flag_use_best : bool, optional\n",
    "            If True, start from the best saved solution; otherwise, use the input text,\n",
    "        flag_shuffle : bool, optional\n",
    "            If not using the best saved solution, shuffle the input words to create an\n",
    "            initial candidate\n",
    "        \"\"\"\n",
    "        self.flag_use_best = flag_use_best\n",
    "        self.flag_shuffle = flag_shuffle\n",
    "\n",
    "        # Instance the scorer, score memos, and record last memo save time.\n",
    "        self.calculator = PerplexityCalculator(model_path=str(PATH_MODEL))\n",
    "        self.score_memo, self.score_memo_with_error = load_score_memo()\n",
    "        self.last_time_score_memo_saved = time()\n",
    "\n",
    "        # Initialize ScoreEstimators for each sample\n",
    "        self.score_estimators = [\n",
    "            ScoreEstimator(\n",
    "                sample_id=sample_id,\n",
    "                epoch=epoch,\n",
    "                device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "            )\n",
    "            # use only the ckp for sample 4, and 5 (the longest samples)\n",
    "            for sample_id, epoch in enumerate([-1, -1, -1, -1, 20, 11]) \n",
    "        ]\n",
    "\n",
    "        # Init to hold current best solutions\n",
    "        self.list_words_best: list[list[str]] = []\n",
    "        self.list_perplexity_best: list[float] = []\n",
    "\n",
    "        for idx in range(NUM_SAMPLES):\n",
    "            if self.flag_use_best:\n",
    "                _, list_words = get_path_words_best(idx)\n",
    "                assert list_words is not None\n",
    "            else:\n",
    "                text: str = DF_SAMPLE.iloc[idx, 1]\n",
    "                list_words = text.split()\n",
    "                if self.flag_shuffle:\n",
    "                    random.shuffle(list_words)\n",
    "\n",
    "            text = \" \".join(list_words)\n",
    "            self.list_words_best.append(list_words.copy())\n",
    "            score_new = self._calc_perplexity(idx, text)\n",
    "            self.list_perplexity_best.append(score_new)\n",
    "\n",
    "            print(f\"idx:{idx} score:{score_new:.4f}\")\n",
    "\n",
    "        # Keep an all-time record of best solutions (for recovery when stuck).\n",
    "        self.list_words_best_all = copy.deepcopy(self.list_words_best)\n",
    "        self.list_perplexity_best_all = copy.deepcopy(self.list_perplexity_best)\n",
    "\n",
    "        # Kick counter for applying kick\n",
    "        self.list_num_kick = [1] * NUM_SAMPLES\n",
    "\n",
    "\n",
    "    def _calc_perplexity(self, n_idx: int, text: Union[str, list[str]]) -> Union[float, list[float]]:\n",
    "        \"\"\"\n",
    "        Calculate the score for a given text or list of texts.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_idx : int\n",
    "            The sample index.\n",
    "        text : str or list[str]\n",
    "            The candidate text(s) to score.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float or list[float]\n",
    "            The perplexity score(s).\n",
    "        \"\"\"\n",
    "        return _get_score(self.calculator, self.score_memo, self.score_memo_with_error, text)\n",
    "\n",
    "\n",
    "    def _get_best(self, n_idx: int) -> tuple[list[str], float]:\n",
    "        \"\"\"\n",
    "        Retrieve the current best solution and its score for a given sample from the class attributes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_idx : int\n",
    "            The problem index.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[list[str], float]\n",
    "            The best word list and its perplexity score.\n",
    "        \"\"\"\n",
    "        return self.list_words_best[n_idx], self.list_perplexity_best[n_idx]\n",
    "\n",
    "\n",
    "    def _update_best_all(self, n_idx: int, words: list[str], perplexity: float):\n",
    "        \"\"\"\n",
    "        Update class attribute of the all-time best solution for a sample if the new candidate is better.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_idx : int\n",
    "            The problem index.\n",
    "        words : list[str]\n",
    "            The candidate word list.\n",
    "        perplexity : float\n",
    "            The candidate perplexity score.\n",
    "        \"\"\"\n",
    "        if perplexity < self.list_perplexity_best_all[n_idx]:\n",
    "            self.list_words_best_all[n_idx] = words.copy()\n",
    "            self.list_perplexity_best_all[n_idx] = perplexity\n",
    "\n",
    "\n",
    "    def _get_best_all(self, n_idx: int) -> tuple[list[str], float]:\n",
    "        \"\"\"\n",
    "        Retrieve the all-time best solution for a given sample.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_idx : int\n",
    "            The problem index.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[list[str], float]\n",
    "            The all-time best word list and its perplexity score.\n",
    "        \"\"\"\n",
    "        return self.list_words_best_all[n_idx], self.list_perplexity_best_all[n_idx]\n",
    "\n",
    "\n",
    "    def _hillclimbing(\n",
    "        self,\n",
    "        n_idx: int,\n",
    "        words_best: list[str],\n",
    "        perplexity_best: float,\n",
    "        score_estimator: ScoreEstimator,\n",
    "        iter_total: int = 500, # set total iterations to run here\n",
    "        pbar=None,\n",
    "        print_every: int = 100,\n",
    "    ) -> tuple[list[str], float]:\n",
    "        \"\"\"\n",
    "        Perform hill-climbing search to find a better solution.\n",
    "\n",
    "        From the current best solution -> recursive DFS on neighbors ->\n",
    "        Use customized threshold greedy selection based on estimated scores.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_idx : int\n",
    "            The problem index.\n",
    "        words_best : list[str]\n",
    "            The current best candidate solution.\n",
    "        perplexity_best : float\n",
    "            The perplexity score of the current best solution.\n",
    "        score_estimator : ScoreEstimator\n",
    "            An instance used to estimate scores and update model parameters.\n",
    "        iter_total : int, optional\n",
    "            Total iterations to try in the search, by default 500.\n",
    "        pbar : tqdm instance, optional\n",
    "            Progress bar instance for updating progress.\n",
    "        print_every : int, optional\n",
    "            Frequency (in iterations) to print status messages, by default 100.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[list[str], float]\n",
    "            The improved word list and its perplexity score.\n",
    "        \"\"\"\n",
    "        iter_count = 0\n",
    "        # Local class to record accepted and rejected candidates.\n",
    "        class Stats:\n",
    "            def __init__(self, max_value: int):\n",
    "                self.max_value = max_value\n",
    "                self.accepted = Counter()\n",
    "                self.rejected = Counter()\n",
    "\n",
    "            def summary(self) -> str:\n",
    "                n_bins = 8\n",
    "                accepted = [0] * n_bins\n",
    "                for value, count in self.accepted.items():\n",
    "                    assert 0 <= value < self.max_value\n",
    "                    accepted[value * n_bins // self.max_value] += count\n",
    "                rejected = [0] * n_bins\n",
    "                for value, count in self.rejected.items():\n",
    "                    assert 0 <= value < self.max_value\n",
    "                    rejected[value * n_bins // self.max_value] += count\n",
    "                return (\n",
    "                    f\" accepted:{accepted}, rejected:{rejected}\"\n",
    "                    f\"     - total:{sum(accepted) + sum(rejected)}\"\n",
    "                )\n",
    "\n",
    "        batch_size = 128\n",
    "        stats = Stats(max_value=batch_size)\n",
    "\n",
    "        visited = set()\n",
    "\n",
    "        def search(words: list[str], depth: int = 0) -> tuple[float, list[str], list[int]]:\n",
    "            \"\"\"\n",
    "            Recursive search to find improved solutions.\n",
    "            \n",
    "            Tracks visited candidates to avoid cycles and applies a depth-dependent\n",
    "            threshold to decide whether to explore a candidate further.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            words : list[str]\n",
    "                Current candidate solution.\n",
    "            depth : int, optional\n",
    "                Current recursion depth, by default 0.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            tuple[float, list[str], list[int]]\n",
    "                The improved perplexity, the corresponding word list, and neighbor\n",
    "                type information.\n",
    "            \"\"\"\n",
    "            nonlocal iter_count\n",
    "            visited.add(tuple(words))\n",
    "\n",
    "            # customized thresholds for different samples; (refer to word-id mapping)\n",
    "            if n_idx == 0:\n",
    "                depth_to_threshold = {\n",
    "                    0: 1.2, 1: 1.12, 2: 1.08, 3: 1.06, 4: 1.04, 5: 1.03, 6: 1.025, 7: 1.02, 8: 1.015, 9: 1.01, 10: 1.01,\n",
    "                    11: 1.01, 12: 1.005, 13: 1.005, 14: 1.002, 15: 1.002, 16: 1.002, 17: 1.001, 18: 1.001, 19: 1.001, 20: 1.0,\n",
    "                }\n",
    "            elif n_idx in [1, 2]:\n",
    "                depth_to_threshold = {\n",
    "                    0: 1.2, 1: 1.12, 2: 1.08, 3: 1.06, 4: 1.04, 5: 1.03, 6: 1.025, 7: 1.02, 8: 1.015, 9: 1.01, 10: 1.01,\n",
    "                    11: 1.01, 12: 1.005, 13: 1.005, 14: 1.002, 15: 1.002, 16: 1.002, 17: 1.001, 18: 1.001, 19: 1.001, 20: 1.0,\n",
    "                }\n",
    "            elif n_idx == 3:\n",
    "                depth_to_threshold = {\n",
    "                    0: 1.1, 1: 1.06, 2: 1.04, 3: 1.03, 4: 1.02, 5: 1.015, 6: 1.01, 7: 1.008, 8: 1.006, 9: 1.005, 10: 1.004,\n",
    "                    11: 1.003, 12: 1.003, 13: 1.002, 14: 1.002, 15: 1.001, 16: 1.001, 17: 1.001, 18: 1.001, 19: 1.001, 20: 1.0,\n",
    "                }\n",
    "            elif n_idx == 4:\n",
    "                depth_to_threshold = {\n",
    "                    0: 1.05, 1: 1.03, 2: 1.02, 3: 1.015, 4: 1.01, 5: 1.008, 6: 1.006, 7: 1.004, 8: 1.003, 9: 1.002, 10: 1.002,\n",
    "                    11: 1.002, 12: 1.002, 13: 1.002, 14: 1.001, 15: 1.001, 16: 1.001, 17: 1.001, 18: 1.001, 19: 1.001, 20: 1.0,\n",
    "                }\n",
    "            elif n_idx == 5:\n",
    "                depth_to_threshold = {\n",
    "                    0: 1.015, 1: 1.01, 2: 1.007, 3: 1.005, 4: 1.004, 5: 1.0035, 6: 1.003, 7: 1.0025, 8: 1.002, 9: 1.0015, 10: 1.001,\n",
    "                    11: 1.001, 12: 1.001, 13: 1.001, 14: 1.001, 15: 1.001, 16: 1.001, 17: 1.001, 18: 1.001, 19: 1.001, 20: 1.0,\n",
    "                }\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid n_idx: {n_idx}\")\n",
    "\n",
    "            # Generate neighbor permutations to evaluate\n",
    "            neighbors = make_neighbors(words)\n",
    "            max_depth = depth\n",
    "\n",
    "            # Continuously generate candidate neighbors\n",
    "            for _ in itertools.count(0):\n",
    "                list_words_nxt: list[list[str]] = []\n",
    "                list_texts_nxt: list[str] = []\n",
    "                list_neighbor_type: list = []\n",
    "\n",
    "                # Allow more candidate for more depth\n",
    "                num_candidates = 2048 if depth < 2 else 4096 if depth < 5 else 8192\n",
    "                while len(list_words_nxt) < num_candidates:\n",
    "                    try:\n",
    "                        words_nxt, neighbor_type = next(neighbors)\n",
    "                        if tuple(words_nxt) in visited:\n",
    "                            continue\n",
    "                        list_words_nxt.append(words_nxt)\n",
    "                        list_texts_nxt.append(\" \".join(words_nxt))\n",
    "                        list_neighbor_type.append(neighbor_type)\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                if len(list_words_nxt) < min(num_candidates, int(1.5 * len(words) ** 2)):\n",
    "                    # return None score improvements, None solution found, None perm meta, max_depth\n",
    "                    return None, None, None, max_depth\n",
    "\n",
    "                # Prune and select the top 112 candidates by estimated score plus 16 random ones.\n",
    "                estimated_scores = score_estimator.estimate_scores(list_texts_nxt)\n",
    "                indices_sorted = np.argsort(estimated_scores).tolist()\n",
    "                indices_keep = indices_sorted[:112] + random.sample(indices_sorted[112:], 16)\n",
    "                assert len(indices_keep) == batch_size\n",
    "\n",
    "                list_words_nxt = [list_words_nxt[i] for i in indices_keep]\n",
    "                list_texts_nxt = [list_texts_nxt[i] for i in indices_keep]\n",
    "                list_neighbor_type = [list_neighbor_type[i] for i in indices_keep]\n",
    "\n",
    "                # Scores candidates\n",
    "                list_perplexity_nxt_with_error = self._calc_perplexity(n_idx, list_texts_nxt)\n",
    "\n",
    "                # pass to nn for online training\n",
    "                score_estimator.update_parameters(list_texts_nxt, list_perplexity_nxt_with_error)\n",
    "\n",
    "                estimated_rank = int(np.argmin(list_perplexity_nxt_with_error))\n",
    "                words_nxt = list_words_nxt[estimated_rank]\n",
    "                perplexity_nxt_with_error = list_perplexity_nxt_with_error[estimated_rank]\n",
    "                neighbor_type = list_neighbor_type[estimated_rank]\n",
    "                    \n",
    "                if perplexity_nxt_with_error < perplexity_best + 2.0:\n",
    "                    perplexity_nxt = self._calc_perplexity(n_idx, \" \".join(words_nxt))\n",
    "                else:\n",
    "                    perplexity_nxt = perplexity_nxt_with_error\n",
    "\n",
    "                iter_count += 1\n",
    "                # If a candidate is strictly better, return it immediately\n",
    "                if perplexity_nxt < perplexity_best:\n",
    "                    stats.accepted[estimated_rank] += 1\n",
    "                    return perplexity_nxt, words_nxt, [neighbor_type], max_depth\n",
    "                \n",
    "                # else, if < customized threshold, search deeper recursively\n",
    "                elif perplexity_nxt < perplexity_best * depth_to_threshold[depth]:\n",
    "                    search_order = list(range(batch_size))\n",
    "                    random.shuffle(search_order)\n",
    "                    for estimated_rank in search_order:\n",
    "                        words_nxt = list_words_nxt[estimated_rank]\n",
    "                        perplexity_nxt = list_perplexity_nxt_with_error[estimated_rank]\n",
    "                        neighbor_type = list_neighbor_type[estimated_rank]\n",
    "\n",
    "                        # track accepted & rejected\n",
    "                        if (perplexity_nxt >= perplexity_best * depth_to_threshold[depth]):\n",
    "                            stats.rejected[estimated_rank] += 1\n",
    "                            continue\n",
    "                        if tuple(words_nxt) in visited:\n",
    "                            continue\n",
    "\n",
    "                        stats.accepted[estimated_rank] += 1\n",
    "                        perplexity_nxt, words_nxt, neighbor_types, max_depth_ = search(words_nxt, depth + 1)\n",
    "                        max_depth = max(max_depth, max_depth_)\n",
    "\n",
    "                        if perplexity_nxt is not None:\n",
    "                            assert perplexity_nxt < perplexity_best\n",
    "                            return (\n",
    "                                perplexity_nxt,\n",
    "                                words_nxt,\n",
    "                                [neighbor_type] + neighbor_types,\n",
    "                                max_depth,\n",
    "                            )\n",
    "                        \n",
    "                # if algo does not produce enough candidates\n",
    "                if iter_count >= iter_total:\n",
    "                    # return None score improvements, None solution found, None perm meta, max_depth\n",
    "                    return None, None, None, max_depth\n",
    "                \n",
    "                # updates progress bar\n",
    "                if iter_count % print_every == 0:\n",
    "                    print(\n",
    "                        f\"[Search] iteration:{iter_count} best:{perplexity_best:.6f}\"\n",
    "                        f\" current:{perplexity_nxt or math.inf:.2f}\"\n",
    "                        f\" neighbor:{neighbor_type}\"\n",
    "                        f\" depth:{depth}\"\n",
    "                        f\" {stats.summary()}\"\n",
    "                    )\n",
    "\n",
    "                # if pbar is not None:\n",
    "                #     pbar.update(1)\n",
    "                \n",
    "        perplexity_nxt, words_nxt, neighbor_types, max_depth = search(words_best)\n",
    "        if perplexity_nxt is not None:\n",
    "            assert perplexity_nxt < perplexity_best\n",
    "            print(\n",
    "                f\"[Search] Update: {perplexity_best:.6f}\"\n",
    "                f\" -> {perplexity_nxt:.2f},\"\n",
    "                f\" neighbor:{','.join(map(str, neighbor_types))}\"\n",
    "                f\" max_depth:{max_depth}\"\n",
    "                f\" {stats.summary()}\"\n",
    "            )\n",
    "            perplexity_best = perplexity_nxt\n",
    "            words_best = words_nxt\n",
    "        else:\n",
    "            print(f\"[Search] No update, max_depth:{max_depth} {stats.summary()}\")\n",
    "\n",
    "        return words_best, perplexity_best\n",
    "\n",
    "\n",
    "    def ILS_kick(self, n_idx: int, words: list[str], n_kick: int = 2) -> tuple[list[str], list[int]]:\n",
    "        \"\"\"\n",
    "        Apply kick to perturb the current solution, hopefully escaping local minimas in subsequent searches.\n",
    "\n",
    "        This function first performs a structured block removal and reinsertion (if n_kick==2),\n",
    "        then applies a series of random swaps based on the problem index-specific strength.\n",
    "        This perturbation is used when hill climbing stagnates.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_idx : int\n",
    "            The problem index.\n",
    "        words : list[str]\n",
    "            The current candidate word list.\n",
    "        n_kick : int, optional\n",
    "            The intensity of the kick, by default 2.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[list[str], list[int]]\n",
    "            The perturbed word list and a list of neighbor type information indicating the moves.\n",
    "        \"\"\"\n",
    "        words = words.copy()\n",
    "        neighbor_types = []\n",
    "\n",
    "        if n_kick == 2:\n",
    "            length = 10\n",
    "            left = random.randint(0, len(words) - length)\n",
    "            right = left + length\n",
    "            removed = words[left:right]\n",
    "            words = words[:left] + words[right:]\n",
    "            neighbor_type = [left]\n",
    "            for word in removed:\n",
    "                insert_idx = random.randint(0, len(words))\n",
    "                words.insert(insert_idx, word)\n",
    "                neighbor_type.append(insert_idx)\n",
    "            neighbor_types.append(tuple(neighbor_type))\n",
    "\n",
    "        # different kicking intensity for different samples\n",
    "        strength = [2, 3, 3, 4, 5, 10] \n",
    "        for _ in range(n_kick * strength[n_idx]):\n",
    "            r0 = random.randint(0, len(words) - 1)\n",
    "            r1 = random.randint(0, len(words) - 1)\n",
    "            words[r0], words[r1] = words[r1], words[r0]\n",
    "            neighbor_types.append((r0, r1))\n",
    "        return words, neighbor_types\n",
    "\n",
    "\n",
    "    def run(self, list_idx_target: Optional[list[int]] = None, print_every: int = 100):\n",
    "        \"\"\"\n",
    "        Main loop to run the optimization process and update/submissions.\n",
    "\n",
    "        Continuously cycles over the target problem indices, performs hill climbing to\n",
    "        improve the current solution, applies kicks if no improvement is observed,\n",
    "        updates the best solutions, and periodically saves the model and score memos.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        list_idx_target : Optional[list[int]], optional\n",
    "            A list of problem indices to target; if None, all problems are processed.\n",
    "        print_every : int, optional\n",
    "            Number of iterations between status print updates, by default 10.\n",
    "        \"\"\"\n",
    "        if list_idx_target is None:\n",
    "            list_idx_target = list(range(NUM_SAMPLES))\n",
    "\n",
    "        outer_iter = 0\n",
    "        for n_idx in itertools.cycle(list_idx_target):\n",
    "            free_memory()\n",
    "            words_best, perplexity_best_old = self._get_best(n_idx)\n",
    "            if outer_iter % print_every == 0:\n",
    "                print(\"-\"*100)\n",
    "                print(f\"[Step] sample_id: {n_idx} Prev: {perplexity_best_old:.6f}\")\n",
    "\n",
    "            # pbar = tqdm(total=500, mininterval=30)\n",
    "            words_best, perplexity_best = self._hillclimbing(\n",
    "                n_idx,\n",
    "                words_best,\n",
    "                perplexity_best_old,\n",
    "                score_estimator=self.score_estimators[n_idx],\n",
    "                iter_total=500,\n",
    "                # pbar=pbar,\n",
    "                print_every=print_every,\n",
    "            )\n",
    "            # pbar.close()\n",
    "            if outer_iter % print_every == 0:\n",
    "                print(f\"    - [Step] sample_id: {n_idx} Current: {perplexity_best:.6f}\")\n",
    "                print(\"-\"*100)\n",
    "\n",
    "            # If hill climbing didn't improve, apply kick\n",
    "            did_kick = False\n",
    "            if perplexity_best_old == perplexity_best:\n",
    "                if words_best == self._get_best_all(n_idx)[0]:\n",
    "                    self.list_num_kick[n_idx] = 0\n",
    "\n",
    "                # Decrement kick counter and reset if necessary\n",
    "                # example: reset + 4 -> 3 -> 2 -> 1 -> reset + 4 -> 3 -> 2 -> 1 -> ...\n",
    "                self.list_num_kick[n_idx] -= 1\n",
    "                flag_reset = self.list_num_kick[n_idx] <= 0\n",
    "\n",
    "                if flag_reset:\n",
    "                    self.list_num_kick[n_idx] = random.randint(2, 3)\n",
    "                n_kick = self.list_num_kick[n_idx]\n",
    "\n",
    "                did_kick = True\n",
    "                if flag_reset:\n",
    "                    print(\"[Step] Reset words\")\n",
    "                    words_best = self._get_best_all(n_idx)[0]\n",
    "                words_best, neighbor_types = self.ILS_kick(n_idx, words_best, n_kick=n_kick)\n",
    "                print(f\"[Step] Apply {n_kick} kicks: {neighbor_types}\")\n",
    "                perplexity_best = self._calc_perplexity(n_idx, \" \".join(words_best))\n",
    "            \n",
    "            # Update current best solution\n",
    "            self.list_words_best[n_idx] = words_best\n",
    "            self.list_perplexity_best[n_idx] = perplexity_best\n",
    "            self._update_best_all(n_idx, words_best, perplexity_best)\n",
    "\n",
    "            # Save submission text if a significant improvement is found\n",
    "            if not did_kick and perplexity_best < self._get_best_all(n_idx)[1] * 1.1:\n",
    "                save_text(self._calc_perplexity, n_idx, \" \".join(words_best), verbose=1)\n",
    "            \n",
    "            # Periodically save score memos and model checkpoints\n",
    "            if time() > self.last_time_score_memo_saved + 1800:\n",
    "                save_score_memo(self.score_memo, self.score_memo_with_error)\n",
    "                self.last_time_score_memo_saved = time()\n",
    "                self.score_estimators[n_idx].save_model()\n",
    "            \n",
    "            outer_iter += 1\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init optimizer\n",
    "optimizer = Optimization(flag_use_best=False, flag_shuffle=True)  # do not use flag_use_best on 1st run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "[Step] sample_id: 0 Prev: 2339.780733\n",
      "[Search] Update: 2339.780733 -> 1109.56, neighbor:(0, 7, 8, 0) max_depth:0  accepted:[0, 0, 1, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "    - [Step] sample_id: 0 Current: 1109.559923\n",
      "----------------------------------------------------------------------------------------------------\n",
      "score:1109.5599\n",
      "[Search] Update: 1109.559923 -> 850.73, neighbor:(3, 5, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:850.7291\n",
      "[Search] Update: 850.729103 -> 847.41, neighbor:(4, 8, 9, 0) max_depth:0  accepted:[0, 0, 1, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:847.4124\n",
      "[Search] Update: 847.412425 -> 780.67, neighbor:(4, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:780.6729\n",
      "[Search] Update: 780.672908 -> 762.59, neighbor:(3, 4, 7, 0),(2, 3, 6, 0) max_depth:1  accepted:[0, 0, 0, 0, 0, 1, 1, 0], rejected:[0, 0, 0, 0, 0, 0, 1, 0]     - total:3\n",
      "score:762.5886\n",
      "[Search] Update: 762.588639 -> 617.56, neighbor:(2, 5, 6, 0),(1, 3, 6, 0),(0, 5, 7, 0) max_depth:4  accepted:[2, 0, 1, 1, 1, 3, 1, 4], rejected:[55, 57, 57, 58, 51, 59, 54, 53]     - total:457\n",
      "score:617.5631\n",
      "[Search] Update: 617.563100 -> 467.99, neighbor:(1, 6, 7, 0) max_depth:0  accepted:[0, 0, 0, 0, 1, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[3, 3, 1, 3, 4, 5, 1, 4], rejected:[124, 123, 126, 125, 124, 123, 127, 123]     - total:1019\n",
      "[Step] Reset words\n",
      "[Step] Apply 3 kicks: [(6, 5), (3, 2), (9, 6), (4, 0), (7, 8), (1, 4)]\n",
      "[Search] Update: 7671.845667 -> 1441.50, neighbor:(0, 2, 7, 0) max_depth:0  accepted:[0, 0, 0, 0, 0, 0, 0, 1], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1441.497092 -> 1209.13, neighbor:(4, 6, 9, 0) max_depth:0  accepted:[0, 0, 0, 0, 1, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1209.129853 -> 912.70, neighbor:(1, 2, 7, 0) max_depth:0  accepted:[0, 0, 0, 0, 0, 0, 0, 1], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 912.699097 -> 824.55, neighbor:(4, 6, 10, 0) max_depth:0  accepted:[0, 0, 0, 0, 0, 1, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 824.554920 -> 796.07, neighbor:(8, 9, 10, 0),(5, 8, 10, 0) max_depth:1  accepted:[0, 0, 0, 0, 1, 0, 0, 1], rejected:[1, 2, 2, 0, 1, 1, 0, 0]     - total:9\n",
      "[Search] Update: 796.070301 -> 792.97, neighbor:(2, 9, 10, 0),(2, 3, 6, 0),(5, 6, 9, 0) max_depth:2  accepted:[0, 0, 0, 0, 0, 1, 0, 2], rejected:[1, 2, 1, 2, 0, 1, 2, 3]     - total:15\n",
      "[Search] Update: 792.966717 -> 762.59, neighbor:(3, 4, 7, 0),(7, 8, 9, 0),(2, 6, 8, 0),(2, 4, 5, 0),(4, 5, 10, 0),(2, 8, 10, 0) max_depth:5  accepted:[0, 1, 0, 2, 0, 1, 0, 5], rejected:[51, 50, 43, 44, 46, 49, 50, 40]     - total:382\n",
      "[Search] Update: 762.588639 -> 467.99, neighbor:(0, 5, 7, 0),(1, 3, 4, 0) max_depth:5  accepted:[3, 2, 1, 6, 8, 5, 5, 7], rejected:[229, 231, 233, 228, 223, 224, 224, 223]     - total:1852\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[6, 5, 1, 4, 1, 3, 3, 3], rejected:[154, 154, 158, 154, 158, 156, 157, 157]     - total:1274\n",
      "[Step] Reset words\n",
      "[Step] Apply 3 kicks: [(3, 1), (9, 0), (8, 4), (1, 2), (5, 7), (6, 7)]\n",
      "[Search] Update: 2130.394601 -> 1317.64, neighbor:(0, 2, 10, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1317.635011 -> 1050.51, neighbor:(2, 5, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1050.510221 -> 960.24, neighbor:(5, 6, 7, 0) max_depth:0  accepted:[0, 0, 0, 1, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 960.244078 -> 857.40, neighbor:(5, 9, 10, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 857.401454 -> 792.97, neighbor:(3, 4, 9, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 792.966717 -> 762.59, neighbor:(2, 3, 4, 0),(7, 9, 10, 0),(2, 3, 5, 0),(3, 4, 5, 0),(8, 9, 10, 0),(2, 8, 10, 0) max_depth:5  accepted:[1, 3, 1, 0, 0, 0, 1, 0], rejected:[16, 12, 16, 14, 13, 16, 19, 16]     - total:128\n",
      "[Search] Update: 762.588639 -> 617.56, neighbor:(1, 2, 5, 0),(0, 5, 7, 0) max_depth:5  accepted:[9, 1, 5, 4, 3, 4, 2, 0], rejected:[217, 223, 219, 219, 223, 226, 227, 227]     - total:1809\n",
      "[Search] Update: 617.563100 -> 467.99, neighbor:(1, 6, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[6, 7, 4, 3, 4, 1, 0, 0], rejected:[105, 100, 107, 109, 107, 111, 112, 112]     - total:888\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 0, 2, 0, 3, 1, 4, 4, 5, 7), (9, 0), (5, 7), (3, 8), (9, 4)]\n",
      "[Search] Update: 5701.231596 -> 1397.15, neighbor:(0, 8, 9, 0) max_depth:0  accepted:[0, 0, 1, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1397.146889 -> 792.97, neighbor:(1, 3, 4, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 792.966717 -> 624.84, neighbor:(3, 5, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 624.842739 -> 538.65, neighbor:(3, 5, 6, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 538.648110 -> 530.30, neighbor:(6, 7, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 530.297145 -> 484.73, neighbor:(4, 9, 10, 0),(3, 5, 9, 0) max_depth:1  accepted:[0, 1, 1, 0, 0, 0, 0, 0], rejected:[1, 0, 0, 0, 0, 0, 0, 0]     - total:3\n",
      "score:484.7308\n",
      "[Search] Update: 484.730831 -> 467.99, neighbor:(4, 5, 10, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[10, 8, 5, 4, 0, 0, 0, 0], rejected:[134, 132, 138, 139, 144, 144, 144, 144]     - total:1146\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 0, 2, 2, 4, 1, 6, 7, 4, 3), (6, 0), (4, 8), (1, 0), (1, 8)]\n",
      "[Search] Update: 9698.121636 -> 1887.43, neighbor:(0, 3, 5, 0) max_depth:0  accepted:[0, 0, 0, 0, 0, 0, 1, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1887.425009 -> 1302.28, neighbor:(2, 6, 8, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1302.284098 -> 1096.63, neighbor:(7, 8, 10, 0) max_depth:0  accepted:[0, 0, 0, 0, 1, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1096.633158 -> 905.60, neighbor:(0, 1, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 905.596416 -> 771.58, neighbor:(0, 5, 7, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 771.577793 -> 596.23, neighbor:(1, 3, 4, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 596.229106 -> 494.29, neighbor:(4, 6, 10, 0) max_depth:0  accepted:[0, 0, 0, 0, 1, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:494.2913\n",
      "[Search] Update: 494.291290 -> 484.73, neighbor:(4, 5, 7, 0),(3, 6, 10, 0) max_depth:4  accepted:[3, 2, 1, 1, 2, 0, 0, 0], rejected:[64, 64, 64, 64, 62, 64, 67, 65]     - total:523\n",
      "score:484.7308\n",
      "[Search] Update: 484.730831 -> 467.99, neighbor:(4, 5, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[ScoreEstimator] Spearman: 0.3687\n",
      "[Search] No update, max_depth:3  accepted:[9, 8, 4, 4, 2, 0, 0, 0], rejected:[116, 118, 122, 123, 126, 128, 128, 128]     - total:1016\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 0, 2, 1, 1, 5, 5, 1, 2, 2), (9, 9), (9, 3), (4, 2), (5, 6)]\n",
      "[Search] Update: 5656.864260 -> 1354.16, neighbor:(0, 5, 6, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1354.161199 -> 654.83, neighbor:(2, 6, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 654.829568 -> 577.89, neighbor:(6, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 577.885065 -> 513.98, neighbor:(4, 7, 9, 0) max_depth:0  accepted:[0, 0, 0, 1, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:513.9816\n",
      "[Search] Update: 513.981617 -> 467.99, neighbor:(2, 3, 4, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[9, 6, 6, 4, 1, 0, 0, 0], rejected:[85, 89, 88, 91, 95, 96, 96, 96]     - total:762\n",
      "[Step] Reset words\n",
      "[Step] Apply 3 kicks: [(2, 7), (6, 4), (7, 6), (6, 5), (2, 7), (0, 5)]\n",
      "[Search] Update: 2155.507018 -> 1046.41, neighbor:(0, 5, 6, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1046.414670 -> 675.62, neighbor:(1, 2, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 675.616090 -> 573.39, neighbor:(2, 3, 10, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 573.387928 -> 518.01, neighbor:(4, 5, 6, 0) max_depth:0  accepted:[0, 0, 1, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 518.012825 -> 513.98, neighbor:(4, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:513.9816\n",
      "[Search] Update: 513.981617 -> 467.99, neighbor:(2, 3, 4, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[9, 10, 4, 3, 1, 0, 0, 0], rejected:[147, 149, 154, 157, 159, 160, 160, 160]     - total:1273\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 0, 1, 3, 4, 2, 4, 0, 2, 0), (4, 6), (9, 3), (4, 4), (7, 1)]\n",
      "[Search] Update: 1487.255122 -> 1118.26, neighbor:(3, 8, 9, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1118.262309 -> 941.67, neighbor:(0, 1, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 941.671276 -> 771.58, neighbor:(0, 4, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 771.577793 -> 509.98, neighbor:(1, 2, 6, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:509.9818\n",
      "[Search] Update: 509.981780 -> 467.99, neighbor:(2, 6, 7, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[10, 8, 5, 1, 1, 0, 0, 0], rejected:[130, 135, 138, 142, 143, 144, 144, 144]     - total:1145\n",
      "[Step] Reset words\n",
      "[Step] Apply 3 kicks: [(7, 6), (6, 2), (8, 2), (8, 9), (6, 4), (2, 7)]\n",
      "[Search] Update: 632.208187 -> 560.11, neighbor:(4, 9, 10, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 560.105411 -> 553.58, neighbor:(6, 7, 8, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 553.579985 -> 528.23, neighbor:(3, 4, 6, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 528.229712 -> 467.99, neighbor:(2, 4, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[12, 8, 5, 1, 0, 0, 0, 0], rejected:[97, 102, 105, 111, 112, 112, 112, 112]     - total:889\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 0, 1, 2, 1, 5, 6, 6, 7, 1), (4, 3), (2, 6), (0, 0), (7, 7)]\n",
      "[Search] Update: 2529.906185 -> 1195.04, neighbor:(0, 5, 6, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1195.043063 -> 716.39, neighbor:(2, 4, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 716.385729 -> 513.98, neighbor:(5, 8, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:513.9816\n",
      "[Search] Update: 513.981617 -> 467.99, neighbor:(2, 3, 4, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[14, 8, 5, 0, 0, 0, 0, 0], rejected:[111, 116, 123, 128, 128, 128, 128, 128]     - total:1017\n",
      "[Step] Reset words\n",
      "[Step] Apply 3 kicks: [(2, 7), (5, 6), (8, 6), (4, 1), (1, 3), (0, 0)]\n",
      "[Search] Update: 2579.804113 -> 642.16, neighbor:(1, 4, 6, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 642.164017 -> 557.92, neighbor:(5, 6, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 557.921767 -> 513.98, neighbor:(2, 6, 7, 0) max_depth:0  accepted:[0, 0, 0, 1, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:513.9816\n",
      "[Search] Update: 513.981617 -> 504.04, neighbor:(7, 9, 10, 0) max_depth:0  accepted:[0, 0, 0, 1, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:504.0403\n",
      "[Search] Update: 504.040313 -> 494.29, neighbor:(8, 9, 10, 0) max_depth:0  accepted:[0, 0, 1, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:494.2913\n",
      "[Search] Update: 494.291290 -> 467.99, neighbor:(7, 8, 9, 0),(2, 3, 4, 0),(5, 7, 10, 0),(2, 3, 8, 0),(5, 6, 10, 0) max_depth:5  accepted:[4, 3, 1, 1, 0, 0, 0, 0], rejected:[44, 46, 45, 47, 48, 52, 46, 45]     - total:382\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[15, 7, 3, 1, 0, 0, 0, 0], rejected:[140, 152, 157, 159, 160, 160, 160, 160]     - total:1274\n",
      "[Step] Reset words\n",
      "[Step] Apply 3 kicks: [(0, 7), (2, 5), (8, 0), (0, 7), (6, 5), (3, 1)]\n",
      "[Search] Update: 1887.425009 -> 733.37, neighbor:(1, 3, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 733.374327 -> 612.76, neighbor:(2, 3, 4, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 612.757186 -> 509.98, neighbor:(7, 8, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:509.9818\n",
      "[Search] Update: 509.981780 -> 467.99, neighbor:(2, 6, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[13, 8, 2, 2, 0, 0, 0, 0], rejected:[125, 136, 141, 142, 144, 144, 144, 144]     - total:1145\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 0, 0, 1, 3, 3, 5, 3, 4, 9), (7, 5), (3, 4), (3, 7), (8, 2)]\n",
      "[Search] Update: 6212.849957 -> 1252.39, neighbor:(0, 2, 3, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1252.394377 -> 805.45, neighbor:(1, 8, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 805.454126 -> 610.37, neighbor:(5, 6, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 610.368272 -> 467.99, neighbor:(4, 7, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[16, 8, 2, 0, 0, 0, 0, 0], rejected:[122, 135, 141, 144, 144, 144, 144, 144]     - total:1144\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 0, 0, 0, 4, 1, 4, 3, 2, 4), (1, 3), (7, 7), (2, 7), (9, 2)]\n",
      "[Search] Update: 2294.525277 -> 1267.16, neighbor:(0, 8, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1267.157206 -> 622.41, neighbor:(1, 8, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 622.406708 -> 555.75, neighbor:(7, 8, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 555.746636 -> 513.98, neighbor:(2, 3, 7, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:513.9816\n",
      "[Search] Update: 513.981617 -> 467.99, neighbor:(2, 3, 4, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[ScoreEstimator] Spearman: 0.8260\n",
      "[Search] No update, max_depth:3  accepted:[14, 9, 2, 0, 0, 0, 0, 0], rejected:[126, 132, 141, 144, 144, 144, 144, 144]     - total:1144\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 0, 2, 1, 0, 1, 5, 4, 7, 0), (7, 6), (5, 3), (6, 9), (4, 1)]\n",
      "[Search] Update: 3273.941861 -> 1711.82, neighbor:(3, 9, 10, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1711.820152 -> 1380.87, neighbor:(1, 6, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1380.869635 -> 1038.27, neighbor:(4, 7, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1038.271406 -> 964.00, neighbor:(1, 8, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Step] sample_id: 0 Prev: 964.002367\n",
      "[Search] Update: 964.002367 -> 467.99, neighbor:(0, 1, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "    - [Step] sample_id: 0 Current: 467.985588\n",
      "----------------------------------------------------------------------------------------------------\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[14, 9, 2, 0, 0, 0, 0, 0], rejected:[125, 134, 141, 144, 144, 144, 144, 144]     - total:1145\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 0, 0, 2, 2, 5, 5, 2, 5, 4), (5, 2), (9, 3), (3, 7), (6, 4)]\n",
      "[Search] Update: 2569.746410 -> 1006.33, neighbor:(0, 3, 4, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1006.327153 -> 629.74, neighbor:(1, 2, 6, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 629.743441 -> 582.42, neighbor:(3, 4, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 582.417474 -> 553.58, neighbor:(2, 8, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 553.579985 -> 504.04, neighbor:(2, 6, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:504.0403\n",
      "[Search] Update: 504.040313 -> 494.29, neighbor:(8, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:494.2913\n",
      "[Search] Update: 494.291290 -> 484.73, neighbor:(5, 7, 8, 0),(6, 8, 10, 0),(3, 4, 8, 0) max_depth:2  accepted:[2, 1, 0, 0, 0, 0, 0, 0], rejected:[1, 0, 2, 3, 1, 0, 1, 3]     - total:14\n",
      "score:484.7308\n",
      "[Search] Update: 484.730831 -> 467.99, neighbor:(4, 5, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[18, 5, 3, 0, 0, 0, 0, 0], rejected:[90, 103, 109, 112, 112, 112, 112, 112]     - total:888\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 0, 2, 1, 3, 4, 4, 3, 5, 4), (8, 1), (7, 3), (4, 3), (9, 1)]\n",
      "[Search] Update: 3148.519118 -> 1042.34, neighbor:(0, 2, 3, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1042.335086 -> 647.20, neighbor:(3, 8, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 647.200572 -> 553.58, neighbor:(5, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 553.579985 -> 538.65, neighbor:(3, 6, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 538.648110 -> 532.37, neighbor:(8, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 532.372669 -> 504.04, neighbor:(5, 8, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:504.0403\n",
      "[Search] Update: 504.040313 -> 494.29, neighbor:(8, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:494.2913\n",
      "[Search] Update: 494.291290 -> 467.99, neighbor:(7, 8, 10, 0),(8, 9, 10, 0),(5, 7, 10, 0),(3, 4, 8, 0),(5, 6, 10, 0) max_depth:4  accepted:[10, 1, 0, 0, 0, 0, 0, 0], rejected:[68, 80, 79, 82, 78, 78, 82, 79]     - total:637\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[16, 8, 2, 0, 0, 0, 0, 0], rejected:[108, 118, 126, 128, 128, 128, 128, 128]     - total:1018\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 1, 2, 0, 2, 0, 3, 1, 5, 4), (6, 9), (3, 7), (6, 9), (0, 6)]\n",
      "[Search] Update: 4763.557300 -> 1022.17, neighbor:(0, 7, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1022.174500 -> 670.36, neighbor:(3, 5, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 670.358404 -> 545.00, neighbor:(3, 6, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 544.997523 -> 530.30, neighbor:(8, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 530.297145 -> 513.98, neighbor:(3, 4, 8, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:513.9816\n",
      "[Search] Update: 513.981617 -> 467.99, neighbor:(5, 6, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[21, 3, 2, 0, 0, 0, 0, 0], rejected:[116, 139, 142, 144, 144, 144, 144, 144]     - total:1143\n",
      "[Step] Reset words\n",
      "[Step] Apply 3 kicks: [(5, 4), (6, 0), (6, 4), (3, 9), (9, 4), (3, 1)]\n",
      "[Search] Update: 1829.355046 -> 1158.28, neighbor:(0, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1158.275454 -> 577.89, neighbor:(1, 4, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 577.885065 -> 509.98, neighbor:(6, 7, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:509.9818\n",
      "[Search] Update: 509.981780 -> 467.99, neighbor:(2, 6, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[17, 8, 0, 0, 0, 0, 0, 0], rejected:[119, 136, 144, 144, 144, 144, 144, 144]     - total:1144\n",
      "[Step] Reset words\n",
      "[Step] Apply 3 kicks: [(2, 3), (4, 2), (4, 7), (1, 9), (8, 6), (4, 3)]\n",
      "[Search] Update: 2189.451314 -> 1223.38, neighbor:(3, 5, 7, 0) max_depth:0  accepted:[0, 0, 0, 0, 0, 0, 1, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1223.382693 -> 837.54, neighbor:(1, 6, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 837.539771 -> 598.56, neighbor:(1, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 598.562681 -> 530.30, neighbor:(3, 4, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 530.297145 -> 504.04, neighbor:(4, 6, 7, 0),(2, 3, 4, 0),(4, 5, 10, 0),(7, 9, 10, 0),(3, 9, 10, 0),(6, 8, 10, 0),(2, 4, 7, 0) max_depth:6  accepted:[6, 1, 0, 0, 0, 0, 0, 0], rejected:[6, 7, 9, 8, 11, 14, 12, 12]     - total:86\n",
      "score:504.0403\n",
      "[Search] Update: 504.040313 -> 467.99, neighbor:(5, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[19, 7, 0, 0, 0, 0, 0, 0], rejected:[104, 120, 128, 128, 128, 128, 128, 128]     - total:1018\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 1, 1, 3, 1, 1, 5, 6, 1, 9), (3, 1), (0, 7), (2, 9), (8, 8)]\n",
      "[Search] Update: 2640.982405 -> 1570.85, neighbor:(1, 4, 9, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1570.854472 -> 1343.62, neighbor:(0, 4, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1343.623033 -> 575.63, neighbor:(1, 3, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 575.632105 -> 538.65, neighbor:(7, 8, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 538.648110 -> 504.04, neighbor:(2, 4, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:504.0403\n",
      "[Search] Update: 504.040313 -> 467.99, neighbor:(5, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[ScoreEstimator] Spearman: 0.9101\n",
      "[Search] No update, max_depth:3  accepted:[19, 4, 1, 1, 0, 0, 0, 0], rejected:[87, 106, 111, 111, 112, 112, 112, 112]     - total:888\n",
      "[Step] Reset words\n",
      "[Step] Apply 3 kicks: [(2, 9), (5, 6), (4, 3), (7, 2), (2, 8), (4, 0)]\n",
      "[Search] Update: 1620.718746 -> 1167.36, neighbor:(2, 5, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1167.359921 -> 956.50, neighbor:(3, 8, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 956.500441 -> 864.13, neighbor:(5, 7, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 864.126137 -> 834.27, neighbor:(3, 6, 7, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 834.274513 -> 814.95, neighbor:(2, 6, 8, 0) max_depth:0  accepted:[0, 0, 0, 0, 0, 1, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 814.948564 -> 762.59, neighbor:(6, 7, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 762.588639 -> 467.99, neighbor:(0, 5, 7, 0),(1, 3, 4, 0) max_depth:1  accepted:[2, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:2\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[20, 6, 1, 0, 0, 0, 0, 0], rejected:[121, 136, 143, 144, 144, 144, 144, 144]     - total:1147\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 1, 0, 1, 1, 5, 5, 7, 6, 1), (0, 4), (6, 6), (5, 0), (7, 1)]\n",
      "[Search] Update: 3124.017148 -> 888.08, neighbor:(0, 5, 6, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 888.080596 -> 566.71, neighbor:(2, 4, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 566.707756 -> 524.12, neighbor:(5, 7, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 524.118996 -> 504.04, neighbor:(2, 3, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:504.0403\n",
      "[Search] Update: 504.040313 -> 494.29, neighbor:(8, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:494.2913\n",
      "[Search] Update: 494.291290 -> 484.73, neighbor:(5, 7, 8, 0),(6, 8, 10, 0),(3, 4, 8, 0) max_depth:2  accepted:[3, 0, 0, 0, 0, 0, 0, 0], rejected:[6, 5, 7, 8, 6, 6, 9, 10]     - total:60\n",
      "score:484.7308\n",
      "[Search] Update: 484.730831 -> 467.99, neighbor:(4, 5, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[18, 6, 1, 0, 0, 0, 0, 0], rejected:[120, 137, 143, 144, 144, 144, 144, 144]     - total:1145\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 1, 0, 3, 3, 3, 0, 2, 0, 3), (2, 2), (6, 7), (3, 2), (6, 0)]\n",
      "[Search] Update: 3567.739568 -> 1659.15, neighbor:(2, 7, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1659.152983 -> 1167.36, neighbor:(0, 1, 4, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1167.359921 -> 923.46, neighbor:(0, 8, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 923.457705 -> 652.28, neighbor:(1, 2, 3, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 652.276629 -> 582.42, neighbor:(3, 5, 8, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 582.417474 -> 530.30, neighbor:(2, 5, 6, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 530.297145 -> 484.73, neighbor:(3, 4, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:484.7308\n",
      "[Search] Update: 484.730831 -> 467.99, neighbor:(4, 5, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[22, 3, 1, 0, 0, 0, 0, 0], rejected:[115, 139, 143, 144, 144, 144, 144, 144]     - total:1143\n",
      "[Step] Reset words\n",
      "[Step] Apply 3 kicks: [(1, 6), (4, 0), (3, 1), (3, 3), (8, 2), (7, 9)]\n",
      "[Search] Update: 10323.596744 -> 1464.20, neighbor:(0, 1, 3, 0) max_depth:0  accepted:[0, 0, 0, 1, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1464.197369 -> 1113.90, neighbor:(3, 5, 6, 0) max_depth:0  accepted:[0, 0, 0, 0, 0, 0, 0, 1], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1113.902618 -> 747.84, neighbor:(0, 5, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 747.838840 -> 577.89, neighbor:(3, 4, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 577.885065 -> 509.98, neighbor:(6, 7, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:509.9818\n",
      "[Search] Update: 509.981780 -> 467.99, neighbor:(2, 6, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[21, 2, 2, 0, 0, 0, 0, 0], rejected:[119, 138, 142, 144, 144, 144, 144, 144]     - total:1144\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 1, 0, 0, 3, 5, 4, 2, 5, 7), (1, 8), (0, 5), (0, 8), (7, 4)]\n",
      "[Search] Update: 5440.153193 -> 1322.79, neighbor:(0, 3, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1322.792088 -> 960.24, neighbor:(1, 6, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 960.244078 -> 627.29, neighbor:(2, 3, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 627.288304 -> 551.42, neighbor:(3, 4, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 551.421782 -> 504.04, neighbor:(2, 5, 6, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:504.0403\n",
      "[Search] Update: 504.040313 -> 467.99, neighbor:(5, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[19, 3, 5, 0, 0, 0, 0, 0], rejected:[121, 140, 139, 144, 144, 144, 144, 144]     - total:1147\n",
      "[Step] Reset words\n",
      "[Step] Apply 3 kicks: [(5, 2), (9, 4), (0, 7), (6, 9), (7, 7), (5, 5)]\n",
      "[Search] Update: 4953.315850 -> 1447.14, neighbor:(0, 3, 9, 0) max_depth:0  accepted:[0, 0, 0, 0, 1, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1447.138952 -> 1118.26, neighbor:(3, 8, 10, 0) max_depth:0  accepted:[0, 0, 1, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1118.262309 -> 1002.40, neighbor:(7, 9, 10, 0) max_depth:0  accepted:[0, 0, 0, 1, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1002.403856 -> 736.24, neighbor:(0, 6, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 736.244673 -> 553.58, neighbor:(2, 4, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 553.579985 -> 538.65, neighbor:(3, 6, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 538.648110 -> 530.30, neighbor:(6, 7, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 530.297145 -> 520.04, neighbor:(6, 7, 8, 0),(6, 7, 10, 0),(5, 6, 9, 0),(6, 9, 10, 0),(7, 8, 9, 0) max_depth:4  accepted:[4, 1, 0, 0, 0, 0, 0, 0], rejected:[6, 11, 13, 10, 13, 9, 11, 12]     - total:90\n",
      "[Search] Update: 520.040270 -> 467.99, neighbor:(3, 4, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[22, 5, 0, 0, 0, 0, 0, 0], rejected:[133, 153, 160, 160, 160, 160, 160, 160]     - total:1273\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 0, 0, 0, 0, 1, 6, 5, 4, 0), (7, 4), (4, 4), (6, 3), (2, 0)]\n",
      "[Search] Update: 7671.845667 -> 2056.80, neighbor:(0, 8, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 2056.799165 -> 860.76, neighbor:(2, 5, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 860.757228 -> 632.21, neighbor:(1, 2, 3, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 632.208187 -> 540.76, neighbor:(2, 5, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 540.756319 -> 513.98, neighbor:(2, 3, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:513.9816\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[Step] sample_id: 0 Prev: 513.981617\n",
      "[Search] Update: 513.981617 -> 504.04, neighbor:(7, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "    - [Step] sample_id: 0 Current: 504.040313\n",
      "----------------------------------------------------------------------------------------------------\n",
      "score:504.0403\n",
      "[Search] Update: 504.040313 -> 494.29, neighbor:(8, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:494.2913\n",
      "[Search] Update: 494.291290 -> 467.99, neighbor:(3, 8, 9, 0),(3, 5, 6, 0),(5, 9, 10, 0) max_depth:2  accepted:[2, 1, 0, 0, 0, 0, 0, 0], rejected:[8, 13, 8, 12, 12, 8, 9, 8]     - total:81\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[15, 11, 1, 0, 0, 0, 0, 0], rejected:[111, 116, 125, 128, 128, 128, 128, 128]     - total:1019\n",
      "[Step] Reset words\n",
      "[Step] Apply 3 kicks: [(5, 0), (4, 7), (9, 2), (5, 3), (3, 9), (7, 6)]\n",
      "[Search] Update: 3223.184097 -> 1522.52, neighbor:(0, 5, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1522.524361 -> 1171.93, neighbor:(2, 3, 4, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1171.928838 -> 952.77, neighbor:(5, 6, 10, 0) max_depth:0  accepted:[0, 0, 1, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 952.771399 -> 821.34, neighbor:(1, 4, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 821.340285 -> 762.59, neighbor:(5, 8, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[ScoreEstimator] Spearman: 0.9118\n",
      "[Search] Update: 762.588639 -> 617.56, neighbor:(1, 2, 5, 0),(0, 5, 7, 0) max_depth:5  accepted:[16, 10, 2, 4, 0, 0, 0, 0], rejected:[213, 217, 226, 225, 229, 230, 232, 232]     - total:1836\n",
      "[Search] Update: 617.563100 -> 467.99, neighbor:(1, 6, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[20, 5, 0, 0, 0, 0, 0, 0], rejected:[134, 154, 160, 160, 160, 160, 160, 160]     - total:1273\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 0, 0, 3, 3, 5, 1, 1, 0, 8), (4, 5), (1, 1), (4, 8), (3, 3)]\n",
      "[Search] Update: 2833.360523 -> 1608.11, neighbor:(1, 2, 8, 0) max_depth:0  accepted:[0, 0, 0, 1, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1608.106212 -> 1386.27, neighbor:(0, 3, 4, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1386.274206 -> 1010.27, neighbor:(0, 3, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1010.265807 -> 919.86, neighbor:(0, 5, 6, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 919.857485 -> 675.62, neighbor:(1, 2, 3, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 675.616090 -> 615.16, neighbor:(6, 7, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 615.155450 -> 566.71, neighbor:(3, 4, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 566.707756 -> 538.65, neighbor:(2, 4, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 538.648110 -> 530.30, neighbor:(6, 7, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 530.297145 -> 520.04, neighbor:(5, 6, 10, 0),(7, 8, 9, 0) max_depth:1  accepted:[2, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 1]     - total:3\n",
      "[Search] Update: 520.040270 -> 467.99, neighbor:(3, 4, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[18, 7, 1, 0, 0, 0, 0, 0], rejected:[104, 119, 127, 128, 128, 128, 128, 128]     - total:1016\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 1, 2, 2, 2, 5, 3, 7, 2, 3), (8, 3), (4, 5), (5, 0), (4, 1)]\n",
      "[Search] Update: 6665.415025 -> 1447.14, neighbor:(0, 2, 3, 0) max_depth:0  accepted:[0, 0, 1, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1447.138952 -> 1199.72, neighbor:(3, 9, 10, 0) max_depth:0  accepted:[0, 0, 0, 0, 0, 1, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1199.720330 -> 1135.87, neighbor:(7, 8, 9, 0) max_depth:0  accepted:[0, 1, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1135.872378 -> 975.37, neighbor:(0, 6, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 975.365722 -> 629.74, neighbor:(3, 4, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 629.743441 -> 538.65, neighbor:(1, 8, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 538.648110 -> 467.99, neighbor:(5, 8, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[17, 7, 1, 0, 0, 0, 0, 0], rejected:[91, 102, 111, 112, 112, 112, 112, 112]     - total:889\n",
      "[Step] Reset words\n",
      "[Step] Apply 3 kicks: [(3, 2), (3, 8), (1, 1), (4, 9), (8, 9), (5, 8)]\n",
      "[Search] Update: 971.563131 -> 617.56, neighbor:(3, 5, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 617.563100 -> 504.04, neighbor:(2, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:504.0403\n",
      "[Search] Update: 504.040313 -> 467.99, neighbor:(5, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[19, 6, 0, 0, 0, 0, 0, 0], rejected:[88, 105, 112, 112, 112, 112, 112, 112]     - total:890\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 0, 2, 0, 1, 2, 0, 2, 6, 5), (7, 3), (0, 1), (3, 4), (5, 2)]\n",
      "[Search] Update: 1430.279273 -> 1006.33, neighbor:(0, 8, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1006.327153 -> 627.29, neighbor:(1, 8, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 627.288304 -> 555.75, neighbor:(4, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 555.746636 -> 536.55, neighbor:(8, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 536.548120 -> 504.04, neighbor:(6, 8, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:504.0403\n",
      "[Search] Update: 504.040313 -> 467.99, neighbor:(5, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[19, 7, 0, 0, 0, 0, 0, 0], rejected:[103, 121, 128, 128, 128, 128, 128, 128]     - total:1018\n",
      "[Step] Reset words\n",
      "[Step] Apply 3 kicks: [(2, 7), (9, 0), (4, 2), (7, 3), (6, 5), (8, 2)]\n",
      "[Search] Update: 2461.666282 -> 1458.49, neighbor:(1, 3, 10, 0) max_depth:0  accepted:[0, 0, 0, 0, 0, 0, 0, 1], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1458.489004 -> 1223.38, neighbor:(0, 7, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1223.382693 -> 702.53, neighbor:(2, 5, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 702.529575 -> 603.26, neighbor:(3, 4, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 603.257266 -> 530.30, neighbor:(2, 7, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 530.297145 -> 513.98, neighbor:(3, 4, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:513.9816\n",
      "[Search] Update: 513.981617 -> 467.99, neighbor:(5, 6, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[19, 7, 0, 0, 0, 0, 0, 0], rejected:[151, 169, 176, 176, 176, 176, 176, 176]     - total:1402\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 0, 0, 0, 4, 0, 2, 3, 1, 6), (4, 3), (6, 2), (8, 6), (4, 5)]\n",
      "[Search] Update: 3027.901245 -> 831.02, neighbor:(0, 6, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 831.021985 -> 649.73, neighbor:(3, 6, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 649.733644 -> 553.58, neighbor:(2, 5, 6, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 553.579985 -> 528.23, neighbor:(3, 4, 6, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 528.229712 -> 467.99, neighbor:(2, 4, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[20, 5, 0, 0, 0, 0, 0, 0], rejected:[117, 138, 144, 144, 144, 144, 144, 144]     - total:1144\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 1, 2, 3, 2, 1, 6, 6, 1, 9), (7, 6), (0, 3), (2, 1), (6, 0)]\n",
      "[Search] Update: 3623.923291 -> 1237.80, neighbor:(0, 3, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1237.803541 -> 659.97, neighbor:(1, 3, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 659.965460 -> 553.58, neighbor:(5, 7, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 553.579985 -> 467.99, neighbor:(2, 4, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[ScoreEstimator] Spearman: 0.9439\n",
      "[Search] No update, max_depth:3  accepted:[19, 6, 0, 0, 0, 0, 0, 0], rejected:[119, 137, 144, 144, 144, 144, 144, 144]     - total:1145\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 0, 1, 1, 0, 0, 4, 7, 4, 3), (1, 1), (3, 0), (2, 7), (4, 1)]\n",
      "[Search] Update: 2040.793027 -> 1122.64, neighbor:(1, 6, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1122.639064 -> 986.86, neighbor:(0, 1, 6, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 986.863024 -> 759.62, neighbor:(0, 2, 3, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 759.615588 -> 566.71, neighbor:(1, 2, 3, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 566.707756 -> 538.65, neighbor:(2, 4, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 538.648110 -> 530.30, neighbor:(6, 7, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 530.297145 -> 484.73, neighbor:(8, 9, 10, 0),(3, 4, 8, 0) max_depth:1  accepted:[2, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 1, 0, 0]     - total:3\n",
      "score:484.7308\n",
      "[Search] Update: 484.730831 -> 467.99, neighbor:(4, 5, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[20, 6, 0, 0, 0, 0, 0, 0], rejected:[120, 136, 144, 144, 144, 144, 144, 144]     - total:1146\n",
      "[Step] Reset words\n",
      "[Step] Apply 3 kicks: [(4, 3), (2, 6), (6, 3), (3, 8), (0, 5), (8, 8)]\n",
      "[Search] Update: 3004.337930 -> 1354.16, neighbor:(0, 5, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1354.161199 -> 680.92, neighbor:(1, 3, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 680.915012 -> 553.58, neighbor:(3, 6, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 553.579985 -> 528.23, neighbor:(3, 4, 6, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 528.229712 -> 467.99, neighbor:(2, 4, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[22, 5, 0, 0, 0, 0, 0, 0], rejected:[116, 137, 144, 144, 144, 144, 144, 144]     - total:1144\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 1, 2, 2, 0, 0, 2, 3, 7, 0), (7, 7), (1, 4), (0, 6), (3, 1)]\n",
      "[Search] Update: 2321.572415 -> 960.24, neighbor:(0, 5, 6, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 960.244078 -> 722.00, neighbor:(3, 7, 8, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 722.004412 -> 632.21, neighbor:(6, 8, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 632.208187 -> 557.92, neighbor:(4, 5, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 557.921767 -> 513.98, neighbor:(2, 6, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:513.9816\n",
      "[Search] Update: 513.981617 -> 504.04, neighbor:(7, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:504.0403\n",
      "[Search] Update: 504.040313 -> 494.29, neighbor:(8, 9, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:494.2913\n",
      "[Search] Update: 494.291290 -> 484.73, neighbor:(5, 7, 8, 0),(6, 9, 10, 0),(6, 9, 10, 0),(3, 4, 8, 0) max_depth:3  accepted:[3, 1, 0, 0, 0, 0, 0, 0], rejected:[4, 7, 3, 5, 12, 7, 9, 5]     - total:56\n",
      "score:484.7308\n",
      "[Search] Update: 484.730831 -> 467.99, neighbor:(4, 5, 10, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:467.9856\n",
      "[Search] No update, max_depth:3  accepted:[21, 5, 0, 0, 0, 0, 0, 0], rejected:[118, 137, 144, 144, 144, 144, 144, 144]     - total:1145\n",
      "[Step] Reset words\n",
      "[Step] Apply 2 kicks: [(0, 0, 0, 1, 2, 4, 0, 4, 5, 3, 0), (4, 1), (4, 9), (6, 3), (0, 6)]\n",
      "[Search] Update: 5397.817585 -> 1498.92, neighbor:(0, 8, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 1498.919808 -> 727.67, neighbor:(1, 3, 7, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 727.667163 -> 582.42, neighbor:(6, 8, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 582.417474 -> 532.37, neighbor:(3, 4, 9, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "[Search] Update: 532.372669 -> 494.29, neighbor:(2, 4, 5, 0) max_depth:0  accepted:[1, 0, 0, 0, 0, 0, 0, 0], rejected:[0, 0, 0, 0, 0, 0, 0, 0]     - total:1\n",
      "score:494.2913\n"
     ]
    }
   ],
   "source": [
    "# run the main loop, adjust in the input to target different samples\n",
    "# leave empty to run on all samples\n",
    "optimizer.run(list_idx_target=[0])\n",
    "# optimizer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
